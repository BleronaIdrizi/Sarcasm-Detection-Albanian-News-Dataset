{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40109a66",
   "metadata": {},
   "source": [
    "# Sarcasm Detection in Albanian News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8491c",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "- Develop a machine learning model (BERT-based) to detect sarcasm in Albanian news articles.\n",
    "- Perform binary classification:  \n",
    "  **Sarcastic (1)** vs **Not Sarcastic (0)**.\n",
    "\n",
    "---\n",
    "\n",
    "--- Challenges\n",
    "\n",
    "- No pre-annotated sarcasm labels exist for Albanian news.\n",
    "- Sarcasm detection requires contextual and semantic understanding.\n",
    "- The dataset is large (~4GB), requiring efficient sampling and preprocessing.\n",
    "- Sarcasm is naturally rare and may lead to class imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "--- Approach\n",
    "\n",
    "--- 1. Data Sampling\n",
    "\n",
    "- Extract a manageable subset (1,500–3,000 articles) for manual annotation.\n",
    "- Apply:\n",
    "  - **Stratified sampling** across categories and sources.\n",
    "  - **Keyword-based filtering** to identify potential sarcasm candidates.\n",
    "  - Include articles from satire domains (e.g., Kungulli) as sarcasm candidates.\n",
    "\n",
    "---\n",
    "\n",
    "--- 2. Annotation Process\n",
    "\n",
    "- Two annotators manually label the selected articles.\n",
    "- Labels:\n",
    "  - `1 = Sarcastic`\n",
    "  - `0 = Not Sarcastic`\n",
    "  - `? = Unsure` (for later review)\n",
    "\n",
    "- Create clear annotation guidelines to ensure consistency.\n",
    "- Perform initial calibration:\n",
    "  - Both annotators label the same 100 samples.\n",
    "  - Compare results and refine guidelines.\n",
    "- Resolve disagreements through discussion.\n",
    "\n",
    "---\n",
    "\n",
    "--- 3. Active Learning (Optional Optimization)\n",
    "\n",
    "- Train a preliminary classifier on early labeled data.\n",
    "- Identify uncertain samples (probability close to 0.5).\n",
    "- Prioritize these samples for annotation.\n",
    "- Iteratively improve dataset quality and model performance.\n",
    "\n",
    "---\n",
    "\n",
    "--- 4. Model Training\n",
    "\n",
    "- Fine-tune a multilingual transformer model:\n",
    "  - **XLM-R**\n",
    "  - or **Multilingual BERT**\n",
    "\n",
    "- Compare against baseline models:\n",
    "  - Logistic Regression\n",
    "  - LinearSVC\n",
    "  - Multinomial Naive Bayes\n",
    "\n",
    "- Use standard NLP preprocessing and tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "--- 5. Evaluation Strategy\n",
    "\n",
    "- Split dataset into:\n",
    "  - 70% Training\n",
    "  - 15% Validation\n",
    "  - 15% Test (held-out set)\n",
    "\n",
    "- Apply stratified splitting to maintain class balance.\n",
    "- Avoid data leakage.\n",
    "- Perform cross-validation during development.\n",
    "\n",
    "- Evaluate using:\n",
    "  - **Precision**\n",
    "  - **Recall**\n",
    "  - **F1-score (Primary Metric)**\n",
    "  - Confusion Matrix\n",
    "  - Accuracy\n",
    "\n",
    "---\n",
    "\n",
    "--- Expected Outcome\n",
    "\n",
    "- A trained sarcasm detection model for Albanian news.\n",
    "- The first manually annotated sarcasm dataset in Albanian news domain.\n",
    "- Performance comparison between:\n",
    "  - Classical machine learning models\n",
    "  - Transformer-based deep learning models\n",
    "- A reproducible research pipeline for future sarcasm detection studies.\n",
    "\n",
    "---\n",
    "\n",
    "--- Project Summary\n",
    "\n",
    "This project aims to build the first sarcasm detection system for Albanian news articles by constructing a manually annotated dataset and applying transformer-based classification methods. The study evaluates both classical machine learning approaches and deep learning architectures to determine the most effective method for detecting sarcasm in low-resource languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "631c5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "863742b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "\n",
    "def print_dataset(text, df):\n",
    "    print(\"\\n\" + text + \":\")\n",
    "    display(df.head())\n",
    "\n",
    "def read_dataset(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3d950",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ee17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_COLUMNS = ['content', 'date', 'title', 'category', 'author', 'source']\n",
    "DF_PATH = \"../data/kosovo_news.csv\"\n",
    "PREPROCESSED_DF_PATH = \"../data/preprocessed_kosovo_news.csv\"\n",
    "SCFA_OUT_FILE  = \"sarcasm_candidates_for_annotation.csv\"\n",
    "TITLE_COL    = \"title\"\n",
    "TEXT_COL     = \"text\"\n",
    "CATEGORY_COL = \"category\"\n",
    "SOURCE_COL   = \"source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d23ae99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As Kate as Meghan; ja cila është princesha më ...</td>\n",
       "      <td>Fun;Argëtim</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>as kate as meghan; ja cila është princesha më ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I kapen 10 kg substanca narkotike në BMW X5, a...</td>\n",
       "      <td>Lajme;Nacionale</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>i kapen 10 kg substanca narkotike në bmw x5, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E fundit, Mbappe mund të zyrtarizohet nesër te...</td>\n",
       "      <td>La Liga;Lajme futbolli;Sport</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>e fundit, mbappe mund të zyrtarizohet nesër te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Enca e quan jetë pushimin në plazh me poza në ...</td>\n",
       "      <td>nan;Entertainment</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>enca e quan jetë pushimin në plazh me poza në ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gurët në veshka – Kurat natyrale dhe si t’i pë...</td>\n",
       "      <td>Lifestyle;Shëndeti</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>gurët në veshka – kurat natyrale dhe si t’i pë...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As Kate as Meghan; ja cila është princesha më ...   \n",
       "1  I kapen 10 kg substanca narkotike në BMW X5, a...   \n",
       "2  E fundit, Mbappe mund të zyrtarizohet nesër te...   \n",
       "3  Enca e quan jetë pushimin në plazh me poza në ...   \n",
       "4  Gurët në veshka – Kurat natyrale dhe si t’i pë...   \n",
       "\n",
       "                       category source  \\\n",
       "0                   Fun;Argëtim  Lajmi   \n",
       "1               Lajme;Nacionale  Lajmi   \n",
       "2  La Liga;Lajme futbolli;Sport  Lajmi   \n",
       "3             nan;Entertainment  Lajmi   \n",
       "4            Lifestyle;Shëndeti  Lajmi   \n",
       "\n",
       "                                                text  \n",
       "0  as kate as meghan; ja cila është princesha më ...  \n",
       "1  i kapen 10 kg substanca narkotike në bmw x5, a...  \n",
       "2  e fundit, mbappe mund të zyrtarizohet nesër te...  \n",
       "3  enca e quan jetë pushimin në plazh me poza në ...  \n",
       "4  gurët në veshka – kurat natyrale dhe si t’i pë...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_dataset(PREPROCESSED_DF_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d79b7",
   "metadata": {},
   "source": [
    "\n",
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d7ecc",
   "metadata": {},
   "source": [
    "### 1. Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "560297e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['title', 'category', 'source', 'text']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRows:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df_ann))\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSources in output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, df_ann[\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m].value_counts().head(\u001b[32m20\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    140\u001b[39m df0 = pd.read_csv(DATA_PATH, nrows=\u001b[32m1\u001b[39m)\n\u001b[32m    141\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mHeader:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(df0.columns))\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m counts = \u001b[43mcount_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m df_ann = sample_balanced_by_source(counts)\n\u001b[32m    146\u001b[39m out_path = get_output_path(DATA_PATH, OUT_FILE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mcount_sources\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     66\u001b[39m source_counts = {}\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(read_chunks()):\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     chunk = \u001b[43mclean_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m0\u001b[39m:\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mclean_chunk\u001b[39m\u001b[34m(chunk, chunk_idx)\u001b[39m\n\u001b[32m     60\u001b[39m chunk = chunk[~chunk[\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m].apply(exclude_source)]\n\u001b[32m     61\u001b[39m chunk = chunk[~chunk[\u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m].apply(is_only_timestamp)]\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m chunk = chunk[~\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.apply(is_junk_title)]\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'title'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"../data/preprocessed_kosovo_news.csv\"\n",
    "OUT_FILE  = \"annotation_dataset_balanced.csv\"\n",
    "\n",
    "TARGET_N = 3000\n",
    "CHUNKSIZE = 50_000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "EXCLUDE_SOURCES = [\"kallxo\", \"kallxo.com\"]\n",
    "TOP_SOURCES = 12\n",
    "\n",
    "def get_output_path(data_path: str, out_file: str) -> str:\n",
    "    folder = os.path.dirname(os.path.abspath(data_path))\n",
    "    return os.path.join(folder, out_file)\n",
    "\n",
    "def is_only_timestamp(x) -> bool:\n",
    "    if pd.isna(x):\n",
    "        return True\n",
    "    s = str(x).strip()\n",
    "    return bool(\n",
    "        re.match(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(Z)?$\", s) or\n",
    "        re.match(r\"^\\d{4}-\\d{2}-\\d{2}$\", s)\n",
    "    )\n",
    "\n",
    "def is_junk_title(x) -> bool:\n",
    "    if pd.isna(x):\n",
    "        return True\n",
    "    s = str(x).strip().lower()\n",
    "    if s in {\"unknown\", \"none\", \"null\", \"nan\", \"\"}:\n",
    "        return True\n",
    "    return len(s.split()) < 4\n",
    "\n",
    "def exclude_source(x) -> bool:\n",
    "    if pd.isna(x):\n",
    "        return False\n",
    "    s = str(x).strip().lower()\n",
    "    return any(bad in s for bad in EXCLUDE_SOURCES)\n",
    "\n",
    "def read_chunks():\n",
    "    return pd.read_csv(\n",
    "        DATA_PATH,\n",
    "        chunksize=CHUNKSIZE,\n",
    "        sep=\",\",\n",
    "        engine=\"python\",\n",
    "        on_bad_lines=\"skip\"\n",
    "    )\n",
    "\n",
    "def clean_chunk(chunk: pd.DataFrame, chunk_idx: int) -> pd.DataFrame:\n",
    "    chunk.columns = chunk.columns.astype(str).str.strip().str.lower()\n",
    "\n",
    "    required = [\"title\", \"text\", \"category\", \"source\"]\n",
    "    if not set(required).issubset(set(chunk.columns)):\n",
    "        print(f\"⚠️ Skipping malformed chunk #{chunk_idx}. Columns:\", list(chunk.columns)[:20])\n",
    "        return pd.DataFrame(columns=required)\n",
    "\n",
    "    chunk = chunk[required].copy()\n",
    "    chunk = chunk[~chunk[\"source\"].apply(exclude_source)]\n",
    "    chunk = chunk[~chunk[\"title\"].apply(is_only_timestamp)]\n",
    "    chunk = chunk[~chunk[\"title\"].apply(is_junk_title)]\n",
    "    return chunk\n",
    "\n",
    "def count_sources():\n",
    "    source_counts = {}\n",
    "    for i, chunk in enumerate(read_chunks()):\n",
    "        chunk = clean_chunk(chunk, i)\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "\n",
    "        vc = chunk[\"source\"].value_counts()\n",
    "        for src, cnt in vc.items():\n",
    "            source_counts[src] = source_counts.get(src, 0) + int(cnt)\n",
    "\n",
    "    counts = pd.Series(source_counts).sort_values(ascending=False)\n",
    "    print(\"✅ Unique sources after cleaning:\", len(counts))\n",
    "    print(\"Top sources:\\n\", counts.head(20))\n",
    "    return counts\n",
    "\n",
    "def sample_balanced_by_source(counts: pd.Series) -> pd.DataFrame:\n",
    "    top = counts.head(TOP_SOURCES)\n",
    "    base_quota = max(1, TARGET_N // len(top))\n",
    "    quota = {src: min(int(cnt), base_quota) for src, cnt in top.items()}\n",
    "\n",
    "    leftover = TARGET_N - sum(quota.values())\n",
    "    if leftover > 0:\n",
    "        for src, cnt in top.items():\n",
    "            cap = int(cnt) - quota[src]\n",
    "            if cap <= 0:\n",
    "                continue\n",
    "            add = min(cap, leftover)\n",
    "            quota[src] += add\n",
    "            leftover -= add\n",
    "            if leftover == 0:\n",
    "                break\n",
    "\n",
    "    collected = {src: [] for src in quota.keys()}\n",
    "    collected_n = {src: 0 for src in quota.keys()}\n",
    "\n",
    "    for i, chunk in enumerate(read_chunks()):\n",
    "        chunk = clean_chunk(chunk, i)\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "\n",
    "        chunk = chunk[chunk[\"source\"].isin(quota.keys())]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "\n",
    "        for src in quota.keys():\n",
    "            need = quota[src] - collected_n[src]\n",
    "            if need <= 0:\n",
    "                continue\n",
    "\n",
    "            sub = chunk[chunk[\"source\"] == src]\n",
    "            if len(sub) == 0:\n",
    "                continue\n",
    "\n",
    "            take = min(need, len(sub))\n",
    "            picked = sub.sample(take, random_state=RANDOM_STATE)\n",
    "            collected[src].append(picked)\n",
    "            collected_n[src] += take\n",
    "\n",
    "        if all(collected_n[s] >= quota[s] for s in quota.keys()):\n",
    "            break\n",
    "\n",
    "    df_out = pd.concat(\n",
    "        [pd.concat(v, ignore_index=True) for v in collected.values() if len(v) > 0],\n",
    "        ignore_index=True\n",
    "    ).drop_duplicates()\n",
    "\n",
    "    df_out = df_out.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    df_out = df_out.head(TARGET_N)\n",
    "\n",
    "    df_out[\"is_sarcasem(1|0|?)\"] = \"\"\n",
    "    return df_out\n",
    "\n",
    "def run():\n",
    "    # sanity check header\n",
    "    df0 = pd.read_csv(DATA_PATH, nrows=1)\n",
    "    print(\"Header:\", list(df0.columns))\n",
    "\n",
    "    counts = count_sources()\n",
    "    df_ann = sample_balanced_by_source(counts)\n",
    "\n",
    "    out_path = get_output_path(DATA_PATH, OUT_FILE)\n",
    "    df_ann.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"\\n✅ Saved: {out_path}\")\n",
    "    print(\"Rows:\", len(df_ann))\n",
    "    print(\"Sources in output:\\n\", df_ann[\"source\"].value_counts().head(20))\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e3d13df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'category', 'source', 'text']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df0 = pd.read_csv(DATA_PATH, nrows=5)\n",
    "print(list(df0.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
