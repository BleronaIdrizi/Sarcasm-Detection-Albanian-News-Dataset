{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40109a66",
   "metadata": {},
   "source": [
    "# Sarcasm Detection in Albanian News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8491c",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "- Develop a machine learning model (BERT-based) to detect sarcasm in Albanian news articles.\n",
    "- Perform binary classification:  \n",
    "  **Sarcastic (1)** vs **Not Sarcastic (0)**.\n",
    "\n",
    "---\n",
    "\n",
    "--- Challenges\n",
    "\n",
    "- No pre-annotated sarcasm labels exist for Albanian news.\n",
    "- Sarcasm detection requires contextual and semantic understanding.\n",
    "- The dataset is large (~4GB), requiring efficient sampling and preprocessing.\n",
    "- Sarcasm is naturally rare and may lead to class imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "--- Approach\n",
    "\n",
    "--- 1. Data Sampling\n",
    "\n",
    "- Extract a manageable subset (1,500–3,000 articles) for manual annotation.\n",
    "- Apply:\n",
    "  - **Stratified sampling** across categories and sources.\n",
    "  - **Keyword-based filtering** to identify potential sarcasm candidates.\n",
    "  - Include articles from satire domains (e.g., Kungulli) as sarcasm candidates.\n",
    "\n",
    "---\n",
    "\n",
    "--- 2. Annotation Process\n",
    "\n",
    "- Two annotators manually label the selected articles.\n",
    "- Labels:\n",
    "  - `1 = Sarcastic`\n",
    "  - `0 = Not Sarcastic`\n",
    "  - `? = Unsure` (for later review)\n",
    "\n",
    "- Create clear annotation guidelines to ensure consistency.\n",
    "- Perform initial calibration:\n",
    "  - Both annotators label the same 100 samples.\n",
    "  - Compare results and refine guidelines.\n",
    "- Resolve disagreements through discussion.\n",
    "\n",
    "---\n",
    "\n",
    "--- 3. Active Learning (Optional Optimization)\n",
    "\n",
    "- Train a preliminary classifier on early labeled data.\n",
    "- Identify uncertain samples (probability close to 0.5).\n",
    "- Prioritize these samples for annotation.\n",
    "- Iteratively improve dataset quality and model performance.\n",
    "\n",
    "---\n",
    "\n",
    "--- 4. Model Training\n",
    "\n",
    "- Fine-tune a multilingual transformer model:\n",
    "  - **XLM-R**\n",
    "  - or **Multilingual BERT**\n",
    "\n",
    "- Compare against baseline models:\n",
    "  - Logistic Regression\n",
    "  - LinearSVC\n",
    "  - Multinomial Naive Bayes\n",
    "\n",
    "- Use standard NLP preprocessing and tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "--- 5. Evaluation Strategy\n",
    "\n",
    "- Split dataset into:\n",
    "  - 70% Training\n",
    "  - 15% Validation\n",
    "  - 15% Test (held-out set)\n",
    "\n",
    "- Apply stratified splitting to maintain class balance.\n",
    "- Avoid data leakage.\n",
    "- Perform cross-validation during development.\n",
    "\n",
    "- Evaluate using:\n",
    "  - **Precision**\n",
    "  - **Recall**\n",
    "  - **F1-score (Primary Metric)**\n",
    "  - Confusion Matrix\n",
    "  - Accuracy\n",
    "\n",
    "---\n",
    "\n",
    "--- Expected Outcome\n",
    "\n",
    "- A trained sarcasm detection model for Albanian news.\n",
    "- The first manually annotated sarcasm dataset in Albanian news domain.\n",
    "- Performance comparison between:\n",
    "  - Classical machine learning models\n",
    "  - Transformer-based deep learning models\n",
    "- A reproducible research pipeline for future sarcasm detection studies.\n",
    "\n",
    "---\n",
    "\n",
    "--- Project Summary\n",
    "\n",
    "This project aims to build the first sarcasm detection system for Albanian news articles by constructing a manually annotated dataset and applying transformer-based classification methods. The study evaluates both classical machine learning approaches and deep learning architectures to determine the most effective method for detecting sarcasm in low-resource languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d79b7",
   "metadata": {},
   "source": [
    "\n",
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9e9bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Loaded labeled rows: 3000\n",
      "labels\n",
      "0    2259\n",
      "1     741\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2400/2400 [00:00<00:00, 6742.62 examples/s]\n",
      "Map: 100%|██████████| 600/600 [00:00<00:00, 6286.29 examples/s]\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 197.43it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]  \n",
      "\u001b[1mDistilBertForSequenceClassification LOAD REPORT\u001b[0m from: distilbert-base-multilingual-cased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "classifier.bias         | MISSING    | \n",
      "classifier.weight       | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "/Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 08:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.753333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "/Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "/Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Metrics (test) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_accuracy': 0.7533333333333333, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 8.8221, 'eval_samples_per_second': 68.011, 'eval_steps_per_second': 68.011, 'epoch': 3.0}\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[452   0]\n",
      " [148   0]]\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7533    1.0000    0.8593       452\n",
      "           1     0.0000    0.0000    0.0000       148\n",
      "\n",
      "    accuracy                         0.7533       600\n",
      "   macro avg     0.3767    0.5000    0.4297       600\n",
      "weighted avg     0.5675    0.7533    0.6474       600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved model to: /Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/models/sarcasm_bert_v1\n",
      "Saved predictions CSV to: /Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/data/sarcasm_labeled_predictions_v1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Lightweight BERT fine-tuning for Sarcasm Detection (Albanian) - MPS friendly\n",
    "# Input CSV columns: content , is_sarcasm (0/1)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ---------------------------\n",
    "# PATHS\n",
    "# ---------------------------\n",
    "REPO_ROOT = Path.cwd()\n",
    "while REPO_ROOT != REPO_ROOT.parent and not (REPO_ROOT / \"data\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "LABELED_FILE = DATA_DIR / \"sarcasm_detection_dataset_v1.csv\"\n",
    "\n",
    "OUT_DIR = REPO_ROOT / \"models\" / \"sarcasm_bert_v1\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PRED_FILE = DATA_DIR / \"sarcasm_labeled_predictions_v1.csv\"\n",
    "\n",
    "# ---------------------------\n",
    "# DEVICE\n",
    "# ---------------------------\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = \"mps\" if use_mps else (\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# MODEL (smaller than xlm-roberta-base)\n",
    "# ---------------------------\n",
    "MODEL_NAME = \"distilbert-base-multilingual-cased\"  # ✅ much smaller, MPS-friendly\n",
    "MAX_LEN = 96\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "\n",
    "# ---------------------------\n",
    "# LOAD DATA\n",
    "# ---------------------------\n",
    "df = pd.read_csv(LABELED_FILE, engine=\"python\", on_bad_lines=\"skip\", dtype={\"content\": str})\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "df[\"content\"] = df[\"content\"].fillna(\"\").astype(str).str.strip()\n",
    "df = df[df[\"content\"] != \"\"].copy()\n",
    "\n",
    "df[\"is_sarcasm\"] = df[\"is_sarcasm\"].fillna(\"\").astype(str).str.strip()\n",
    "df = df[df[\"is_sarcasm\"].isin([\"0\", \"1\"])].copy()\n",
    "df[\"labels\"] = df[\"is_sarcasm\"].astype(int)\n",
    "\n",
    "print(\"Loaded labeled rows:\", len(df))\n",
    "print(df[\"labels\"].value_counts())\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df[[\"content\", \"labels\"]],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"labels\"],\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# ---------------------------\n",
    "# TOKENIZE\n",
    "# ---------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"content\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "train_ds = train_ds.map(tokenize_batch, batched=True)\n",
    "test_ds  = test_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "keep = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep])\n",
    "test_ds  = test_ds.remove_columns([c for c in test_ds.column_names if c not in keep])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ---------------------------\n",
    "# MODEL LOAD (fp16 on MPS helps memory)\n",
    "# ---------------------------\n",
    "torch_dtype = torch.float16 if device == \"mps\" else torch.float32\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch_dtype,\n",
    ")\n",
    "\n",
    "# Enable checkpointing (saves memory during training)\n",
    "try:\n",
    "    model.gradient_checkpointing_enable()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------------------\n",
    "# METRICS\n",
    "# ---------------------------\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# ---------------------------\n",
    "# TRAINING ARGS\n",
    "# ---------------------------\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    fp16=False,   # leave False on MPS\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# TRAIN\n",
    "# ---------------------------\n",
    "trainer.train()\n",
    "\n",
    "# ---------------------------\n",
    "# EVAL\n",
    "# ---------------------------\n",
    "pred_out = trainer.predict(test_ds)\n",
    "logits = pred_out.predictions\n",
    "y_true = pred_out.label_ids\n",
    "y_pred = np.argmax(logits, axis=1)\n",
    "\n",
    "print(\"\\n=== Metrics (test) ===\")\n",
    "print(trainer.evaluate(test_ds))\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# ---------------------------\n",
    "# SAVE\n",
    "# ---------------------------\n",
    "trainer.save_model(str(OUT_DIR))\n",
    "tokenizer.save_pretrained(str(OUT_DIR))\n",
    "print(\"\\nSaved model to:\", OUT_DIR)\n",
    "\n",
    "probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "conf = probs.max(axis=1)\n",
    "\n",
    "pred_df = test_df.copy()\n",
    "pred_df[\"pred_label\"] = y_pred\n",
    "pred_df[\"confidence\"] = conf\n",
    "pred_df.to_csv(PRED_FILE, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved predictions CSV to:\", PRED_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
