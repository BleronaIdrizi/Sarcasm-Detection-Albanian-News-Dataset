{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd8491c",
   "metadata": {},
   "source": [
    "## Sarcasm Detection â€“ Dataset Preparation & Annotation Workflow\n",
    "\n",
    "This notebook prepares the dataset for sarcasm detection experiments.\n",
    "It includes dataset merging, annotation setup, quality control, and final dataset preparation for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a855f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356b5d7",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de09e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path().resolve().parents[1]\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "PREPROCESSED_DF = DATA_DIR / \"preprocessed_kosovo_news.csv\"\n",
    "PREPARED_FOR_ANNOTATION_DF = DATA_DIR / \"annotation_kungulli_3k.csv\"\n",
    "ANNOTATED_DF = DATA_DIR / \"annotation_kungulli_3k_with_sarcasm.csv\"\n",
    "PREPARED_LABELED_DF = DATA_DIR / \"sarcasm_detection_dataset_v1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4663fb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/data/preprocessed_kosovo_news.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kur Beatrice Borromeo u martua me Pierre Casir...</td>\n",
       "      <td>As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...</td>\n",
       "      <td>Fun;ArgÃ«tim</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GjatÃ« kontrollit tÃ« automjetit zyrtarÃ«t polico...</td>\n",
       "      <td>I kapen 10 kg substanca narkotike nÃ« BMW X5, a...</td>\n",
       "      <td>Lajme;Nacionale</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>I kapen 10 kg substanca narkotike nÃ« BMW X5, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enca Haxhia njihet si njÃ« ndÃ«r kÃ«ngÃ«taret mÃ« s...</td>\n",
       "      <td>Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GurÃ«t nÃ« veshka janÃ« depozitime minerale qÃ« fo...</td>\n",
       "      <td>GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...</td>\n",
       "      <td>Lifestyle;ShÃ«ndeti</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NÃ« vendim thuhet se Zyra e Prokurorit tÃ« Speci...</td>\n",
       "      <td>Trupi gjykues ua vazhdon paraburgimin Nasim Ha...</td>\n",
       "      <td>Lajme;Nacionale</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>Trupi gjykues ua vazhdon paraburgimin Nasim Ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Kur Beatrice Borromeo u martua me Pierre Casir...   \n",
       "1  GjatÃ« kontrollit tÃ« automjetit zyrtarÃ«t polico...   \n",
       "2  Enca Haxhia njihet si njÃ« ndÃ«r kÃ«ngÃ«taret mÃ« s...   \n",
       "3  GurÃ«t nÃ« veshka janÃ« depozitime minerale qÃ« fo...   \n",
       "4  NÃ« vendim thuhet se Zyra e Prokurorit tÃ« Speci...   \n",
       "\n",
       "                                               title            category  \\\n",
       "0  As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...         Fun;ArgÃ«tim   \n",
       "1  I kapen 10 kg substanca narkotike nÃ« BMW X5, a...     Lajme;Nacionale   \n",
       "2  Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...       Entertainment   \n",
       "3  GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...  Lifestyle;ShÃ«ndeti   \n",
       "4  Trupi gjykues ua vazhdon paraburgimin Nasim Ha...     Lajme;Nacionale   \n",
       "\n",
       "  source                                               text  \n",
       "0  Lajmi  As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...  \n",
       "1  Lajmi  I kapen 10 kg substanca narkotike nÃ« BMW X5, a...  \n",
       "2  Lajmi  Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...  \n",
       "3  Lajmi  GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...  \n",
       "4  Lajmi  Trupi gjykues ua vazhdon paraburgimin Nasim Ha...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(PREPROCESSED_DF)\n",
    "df = pd.read_csv(PREPROCESSED_DF)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d7ecc",
   "metadata": {},
   "source": [
    "### 1. Data Sampling\n",
    "\n",
    "- Load preprocessed dataset\n",
    "- Merge Kungulli sample with other sources\n",
    "- Create final annotation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3c76de5f946aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T17:40:39.172398Z",
     "start_time": "2026-02-19T17:22:19.531739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['content', 'title', 'category', 'source', 'text']\n",
      "Kungulli found: 694\n",
      "Other sources found: 1454473\n",
      "\n",
      "âœ… Saved: /Users/fatlumberisha/Blerona/Sarcasm-Detection-Albanian-News-Dataset/data/annotation_kungulli_3k.csv\n",
      "Total rows: 3000\n",
      "Kungulli in final: 694\n"
     ]
    }
   ],
   "source": [
    "TARGET_N = 3000\n",
    "CHUNKSIZE = 50_000\n",
    "RANDOM_STATE = 42\n",
    "# DEBUG_LIMIT = 4   # None pÃ«r full run\n",
    "\n",
    "SATIRE_DOMAINS = {\"kungulli.com\", \"www.kungulli.com\"}\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = df.columns.astype(str).str.strip().str.lower()\n",
    "    return df\n",
    "\n",
    "def get_output_path(data_path: str, out_file: str) -> str:\n",
    "    folder = os.path.dirname(os.path.abspath(data_path))\n",
    "    return os.path.join(folder, out_file)\n",
    "\n",
    "def extract_domain(url: str) -> str:\n",
    "    if pd.isna(url):\n",
    "        return \"\"\n",
    "    s = str(url).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    if not re.match(r\"^https?://\", s, flags=re.IGNORECASE):\n",
    "        s = \"https://\" + s\n",
    "    try:\n",
    "        host = urlparse(s).netloc.lower()\n",
    "        host = host.split(\":\")[0]\n",
    "        return host\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def read_chunks(path: str):\n",
    "    try:\n",
    "        return pd.read_csv(\n",
    "            path,\n",
    "            chunksize=CHUNKSIZE,\n",
    "            sep=\",\",\n",
    "            engine=\"c\",\n",
    "            on_bad_lines=\"skip\",\n",
    "        )\n",
    "    except Exception:\n",
    "        return pd.read_csv(\n",
    "            path,\n",
    "            chunksize=CHUNKSIZE,\n",
    "            sep=\",\",\n",
    "            engine=\"python\",\n",
    "            on_bad_lines=\"skip\",\n",
    "        )\n",
    "\n",
    "def ensure_source_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    if \"source\" in cols:\n",
    "        df[\"source\"] = df[\"source\"].astype(str).str.strip().str.lower()\n",
    "        return df\n",
    "\n",
    "    if \"url\" in cols:\n",
    "        df[\"source\"] = df[\"url\"].apply(extract_domain)\n",
    "        return df\n",
    "\n",
    "    raise ValueError(\n",
    "        \"The dataset has neither 'source' nor 'url'. I can't find pumpkin without either of them.\"\n",
    "    )\n",
    "\n",
    "def filter_kungulli(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = ensure_source_column(df)\n",
    "    df[\"source\"] = df[\"source\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    mask = df[\"source\"].isin(SATIRE_DOMAINS) | df[\"source\"].str.contains(\"kungulli\", na=False)\n",
    "    return df[mask].copy()\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def run():\n",
    "    df0 = pd.read_csv(PREPROCESSED_DF, nrows=1)\n",
    "    df0 = normalize_cols(df0)\n",
    "    print(\"Header:\", list(df0.columns))\n",
    "\n",
    "    kungulli_rows = []\n",
    "    other_rows = []\n",
    "\n",
    "    for chunk in read_chunks(PREPROCESSED_DF):\n",
    "        chunk = normalize_cols(chunk)\n",
    "\n",
    "        try:\n",
    "            k = filter_kungulli(chunk)\n",
    "        except ValueError as e:\n",
    "            print(\"âŒ\", str(e))\n",
    "            return\n",
    "\n",
    "        # kungulli\n",
    "        if len(k) > 0:\n",
    "            kungulli_rows.append(k)\n",
    "\n",
    "        # not kungulli\n",
    "        chunk = ensure_source_column(chunk)\n",
    "        chunk[\"source\"] = chunk[\"source\"].astype(str).str.lower().str.strip()\n",
    "        mask_other = ~(\n",
    "            chunk[\"source\"].isin(SATIRE_DOMAINS) |\n",
    "            chunk[\"source\"].str.contains(\"kungulli\", na=False)\n",
    "        )\n",
    "        o = chunk[mask_other]\n",
    "        if len(o) > 0:\n",
    "            other_rows.append(o)\n",
    "\n",
    "    if not kungulli_rows:\n",
    "        print(\"No kungulli found.\")\n",
    "        return\n",
    "\n",
    "    df_k = pd.concat(kungulli_rows, ignore_index=True).drop_duplicates()\n",
    "    df_o = pd.concat(other_rows, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    print(\"Kungulli found:\", len(df_k))\n",
    "    print(\"Other sources found:\", len(df_o))\n",
    "\n",
    "    # -------------------------\n",
    "    # Build 3K total\n",
    "    # -------------------------\n",
    "    needed_from_other = TARGET_N - len(df_k)\n",
    "\n",
    "    if needed_from_other <= 0:\n",
    "        df_out = df_k.sample(n=TARGET_N, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        df_o_sample = df_o.sample(\n",
    "            n=min(needed_from_other, len(df_o)),\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        df_out = pd.concat([df_k, df_o_sample], ignore_index=True)\n",
    "\n",
    "    # Shuffle final\n",
    "    df_out = df_out.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "    df_out[\"is_sarcasm(1|0|?)\"] = \"\"\n",
    "\n",
    "    out_path = get_output_path(PREPROCESSED_DF, PREPARED_FOR_ANNOTATION_DF)\n",
    "    df_out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\nâœ… Saved:\", out_path)\n",
    "    print(\"Total rows:\", len(df_out))\n",
    "    print(\"Kungulli in final:\", sum(df_out[\"source\"].str.contains(\"kungulli\")))\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88781e8164bb92e6",
   "metadata": {},
   "source": [
    "### 2. Claude-Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69b31444ca97eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T20:11:05.091828Z",
     "start_time": "2026-02-19T18:17:49.865107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_CSV: /Users/fatlumberisha/Blerona/Sarcasm-Detection-Albanian-News-Dataset/data/annotation_kungulli_3k.csv\n",
      "OUTPUT_CSV: /Users/fatlumberisha/Blerona/Sarcasm-Detection-Albanian-News-Dataset/data/annotation_kungulli_3k_with_sarcasm.csv\n",
      "Processed 0/3000\n",
      "Processed 50/3000\n",
      "ðŸ’¾ Saved progress at row 99\n",
      "Processed 100/3000\n",
      "Processed 150/3000\n",
      "ðŸ’¾ Saved progress at row 199\n",
      "Processed 200/3000\n",
      "Processed 250/3000\n",
      "ðŸ’¾ Saved progress at row 299\n",
      "Processed 300/3000\n",
      "Processed 350/3000\n",
      "ðŸ’¾ Saved progress at row 399\n",
      "Processed 400/3000\n",
      "Processed 450/3000\n",
      "ðŸ’¾ Saved progress at row 499\n",
      "Processed 500/3000\n",
      "Processed 550/3000\n",
      "ðŸ’¾ Saved progress at row 599\n",
      "Processed 600/3000\n",
      "Processed 650/3000\n",
      "ðŸ’¾ Saved progress at row 699\n",
      "Processed 700/3000\n",
      "âš ï¸ Error at row 702: No valid JSON found in model output: ...\n",
      "Processed 750/3000\n",
      "ðŸ’¾ Saved progress at row 800\n",
      "Processed 800/3000\n",
      "Processed 850/3000\n",
      "ðŸ’¾ Saved progress at row 900\n",
      "Processed 900/3000\n",
      "Processed 950/3000\n",
      "ðŸ’¾ Saved progress at row 1000\n",
      "Processed 1000/3000\n",
      "Processed 1050/3000\n",
      "ðŸ’¾ Saved progress at row 1100\n",
      "Processed 1100/3000\n",
      "Processed 1150/3000\n",
      "ðŸ’¾ Saved progress at row 1200\n",
      "Processed 1200/3000\n",
      "Processed 1250/3000\n",
      "âš ï¸ Error at row 1272: No valid JSON found in model output: ...\n",
      "Processed 1300/3000\n",
      "ðŸ’¾ Saved progress at row 1301\n",
      "Processed 1350/3000\n",
      "Processed 1400/3000\n",
      "ðŸ’¾ Saved progress at row 1401\n",
      "Processed 1450/3000\n",
      "Processed 1500/3000\n",
      "ðŸ’¾ Saved progress at row 1501\n",
      "Processed 1550/3000\n",
      "Processed 1600/3000\n",
      "ðŸ’¾ Saved progress at row 1601\n",
      "Processed 1650/3000\n",
      "Processed 1700/3000\n",
      "ðŸ’¾ Saved progress at row 1701\n",
      "Processed 1750/3000\n",
      "Processed 1800/3000\n",
      "ðŸ’¾ Saved progress at row 1801\n",
      "Processed 1850/3000\n",
      "Processed 1900/3000\n",
      "ðŸ’¾ Saved progress at row 1901\n",
      "Processed 1950/3000\n",
      "Processed 2000/3000\n",
      "ðŸ’¾ Saved progress at row 2001\n",
      "Processed 2050/3000\n",
      "Processed 2100/3000\n",
      "ðŸ’¾ Saved progress at row 2101\n",
      "Processed 2150/3000\n",
      "Processed 2200/3000\n",
      "ðŸ’¾ Saved progress at row 2201\n",
      "Processed 2250/3000\n",
      "Processed 2300/3000\n",
      "ðŸ’¾ Saved progress at row 2301\n",
      "Processed 2350/3000\n",
      "âš ï¸ Error at row 2358: No valid JSON found in model output: ...\n",
      "Processed 2400/3000\n",
      "ðŸ’¾ Saved progress at row 2402\n",
      "Processed 2450/3000\n",
      "Processed 2500/3000\n",
      "ðŸ’¾ Saved progress at row 2502\n",
      "Processed 2550/3000\n",
      "Processed 2600/3000\n",
      "ðŸ’¾ Saved progress at row 2602\n",
      "Processed 2650/3000\n",
      "Processed 2700/3000\n",
      "ðŸ’¾ Saved progress at row 2702\n",
      "Processed 2750/3000\n",
      "Processed 2800/3000\n",
      "ðŸ’¾ Saved progress at row 2802\n",
      "Processed 2850/3000\n",
      "Processed 2900/3000\n",
      "ðŸ’¾ Saved progress at row 2902\n",
      "Processed 2950/3000\n",
      "âœ… Done. All rows processed.\n"
     ]
    }
   ],
   "source": [
    "MODEL = os.getenv(\"CLAUDE_MODEL\", \"claude-opus-4-6\")\n",
    "API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "MAX_CHARS = int(os.getenv(\"MAX_CHARS\", \"2500\"))\n",
    "SLEEP_BETWEEN = float(os.getenv(\"SLEEP_BETWEEN\", \"0.1\"))\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def truncate_text(s: str, max_chars: int) -> str:\n",
    "    s = str(s).strip()\n",
    "    if len(s) <= max_chars:\n",
    "        return s\n",
    "    return s[: max_chars - 50] + \"\\n\\n[TRUNCATED]\"\n",
    "\n",
    "def backoff_sleep(attempt: int) -> None:\n",
    "    base = min(2 ** attempt, 20)\n",
    "    jitter = random.uniform(0, 0.5)\n",
    "    time.sleep(base + jitter)\n",
    "\n",
    "def extract_json(text: str) -> dict:\n",
    "    text = (text or \"\").strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return json.loads(text[start:end + 1])\n",
    "\n",
    "    raise ValueError(f\"No valid JSON found in model output: {text[:200]}...\")\n",
    "\n",
    "def read_dataset(path: Path) -> pd.DataFrame:\n",
    "    # robust encoding fallback (du hattest UTF-8 Probleme)\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "def detect_sarcasm(client: anthropic.Anthropic, text: str) -> int:\n",
    "    system = (\n",
    "        \"You are a sarcasm detector for Albanian-language news texts.\\n\"\n",
    "        \"Label sarcasm as 1 if the text uses irony, mockery, sarcasm, or clearly says the opposite of intent.\\n\"\n",
    "        \"Otherwise label 0.\\n\"\n",
    "        \"If uncertain, choose 0.\\n\"\n",
    "        \"Return ONLY valid JSON exactly in this format:\\n\"\n",
    "        '{\"sarcasm\": 0 or 1, \"confidence\": 0.0-1.0}\\n'\n",
    "        \"No extra text.\"\n",
    "    )\n",
    "\n",
    "    user_text = truncate_text(text, MAX_CHARS)\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            msg = client.messages.create(\n",
    "                model=MODEL,\n",
    "                max_tokens=80,\n",
    "                temperature=0.0,\n",
    "                system=system,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_text}],\n",
    "            )\n",
    "            # msg.content ist Liste von Content-BlÃ¶cken; typischerweise 1 Textblock\n",
    "            out_text = \"\".join(block.text for block in msg.content if getattr(block, \"type\", \"\") == \"text\")\n",
    "            data = extract_json(out_text)\n",
    "            return int(data.get(\"sarcasm\", 0))\n",
    "        except Exception:\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                raise\n",
    "            backoff_sleep(attempt)\n",
    "\n",
    "    return 0\n",
    "\n",
    "def main():\n",
    "    if not API_KEY:\n",
    "        raise SystemExit(\"Missing ANTHROPIC_API_KEY in .env\")\n",
    "\n",
    "    print(\"INPUT_CSV:\", PREPARED_FOR_ANNOTATION_DF)\n",
    "    print(\"OUTPUT_CSV:\", ANNOTATED_DF)\n",
    "\n",
    "    df = read_dataset(PREPARED_FOR_ANNOTATION_DF)\n",
    "\n",
    "    if \"content\" not in df.columns:\n",
    "        raise ValueError(\"The column 'content' does not exist.\")\n",
    "\n",
    "    # Create sarcasm column if missing\n",
    "    if \"sarcasm\" not in df.columns:\n",
    "        df[\"sarcasm\"] = None\n",
    "\n",
    "    client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "    total = len(df)\n",
    "    processed_since_save = 0\n",
    "\n",
    "    for i in range(total):\n",
    "        # Skip if already processed\n",
    "        if pd.notna(df.loc[i, \"sarcasm\"]):\n",
    "            continue\n",
    "\n",
    "        text = str(df.loc[i, \"content\"]).strip()\n",
    "        if not text:\n",
    "            df.loc[i, \"sarcasm\"] = 0\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            label = detect_sarcasm(client, text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at row {i}: {e}\")\n",
    "            df.loc[i, \"sarcasm\"] = 0\n",
    "            continue\n",
    "\n",
    "        df.loc[i, \"sarcasm\"] = label\n",
    "        processed_since_save += 1\n",
    "\n",
    "        # Save every 100 rows\n",
    "        if processed_since_save >= 100:\n",
    "            df.to_csv(ANNOTATED_DF, index=False, encoding=\"utf-8\")\n",
    "            print(f\"ðŸ’¾ Saved progress at row {i}\")\n",
    "            processed_since_save = 0\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processed {i}/{total}\")\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    # Final save\n",
    "    df.to_csv(ANNOTATED_DF, index=False, encoding=\"utf-8\")\n",
    "    print(\"âœ… Done. All rows processed.\")\n",
    "\n",
    "main()\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "for m in client.models.list():\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea966afd",
   "metadata": {},
   "source": [
    "# OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0920dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming existing progress file: /Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/data/preprocessed_kosovo_news_with_is_sarcasm_v1.csv\n",
      "Already labeled -> 1s:751, 0s:116949, total:120000\n",
      "Rows to label (missing): 2300\n",
      "Labeled 50/2300 | last_row=117749 | 1s:751 0s:116999\n",
      "ðŸ’¾ Saved | last_row=117799 | 1s:751 0s:117049 | labeled_in_run:100\n",
      "Labeled 100/2300 | last_row=117799 | 1s:751 0s:117049\n",
      "Labeled 150/2300 | last_row=117849 | 1s:751 0s:117099\n",
      "ðŸ’¾ Saved | last_row=117899 | 1s:751 0s:117149 | labeled_in_run:200\n",
      "Labeled 200/2300 | last_row=117899 | 1s:751 0s:117149\n",
      "Labeled 250/2300 | last_row=117949 | 1s:751 0s:117199\n",
      "ðŸ’¾ Saved | last_row=117999 | 1s:751 0s:117249 | labeled_in_run:300\n",
      "Labeled 300/2300 | last_row=117999 | 1s:751 0s:117249\n",
      "Labeled 350/2300 | last_row=118049 | 1s:753 0s:117297\n",
      "ðŸ’¾ Saved | last_row=118099 | 1s:753 0s:117347 | labeled_in_run:400\n",
      "Labeled 400/2300 | last_row=118099 | 1s:753 0s:117347\n",
      "Labeled 450/2300 | last_row=118149 | 1s:754 0s:117396\n",
      "ðŸ’¾ Saved | last_row=118199 | 1s:756 0s:117444 | labeled_in_run:500\n",
      "Labeled 500/2300 | last_row=118199 | 1s:756 0s:117444\n",
      "Labeled 550/2300 | last_row=118249 | 1s:756 0s:117494\n",
      "ðŸ’¾ Saved | last_row=118299 | 1s:756 0s:117544 | labeled_in_run:600\n",
      "Labeled 600/2300 | last_row=118299 | 1s:756 0s:117544\n",
      "Labeled 650/2300 | last_row=118349 | 1s:757 0s:117593\n",
      "ðŸ’¾ Saved | last_row=118399 | 1s:757 0s:117643 | labeled_in_run:700\n",
      "Labeled 700/2300 | last_row=118399 | 1s:757 0s:117643\n",
      "Labeled 750/2300 | last_row=118449 | 1s:758 0s:117692\n",
      "ðŸ’¾ Saved | last_row=118499 | 1s:758 0s:117742 | labeled_in_run:800\n",
      "Labeled 800/2300 | last_row=118499 | 1s:758 0s:117742\n",
      "Labeled 850/2300 | last_row=118549 | 1s:758 0s:117792\n",
      "ðŸ’¾ Saved | last_row=118599 | 1s:759 0s:117841 | labeled_in_run:900\n",
      "Labeled 900/2300 | last_row=118599 | 1s:759 0s:117841\n",
      "Labeled 950/2300 | last_row=118649 | 1s:759 0s:117891\n",
      "ðŸ’¾ Saved | last_row=118699 | 1s:759 0s:117941 | labeled_in_run:1000\n",
      "Labeled 1000/2300 | last_row=118699 | 1s:759 0s:117941\n",
      "Labeled 1050/2300 | last_row=118749 | 1s:759 0s:117991\n",
      "ðŸ’¾ Saved | last_row=118799 | 1s:759 0s:118041 | labeled_in_run:1100\n",
      "Labeled 1100/2300 | last_row=118799 | 1s:759 0s:118041\n",
      "Labeled 1150/2300 | last_row=118849 | 1s:759 0s:118091\n",
      "ðŸ’¾ Saved | last_row=118899 | 1s:760 0s:118140 | labeled_in_run:1200\n",
      "Labeled 1200/2300 | last_row=118899 | 1s:760 0s:118140\n",
      "Labeled 1250/2300 | last_row=118949 | 1s:760 0s:118190\n",
      "ðŸ’¾ Saved | last_row=118999 | 1s:760 0s:118240 | labeled_in_run:1300\n",
      "Labeled 1300/2300 | last_row=118999 | 1s:760 0s:118240\n",
      "Labeled 1350/2300 | last_row=119049 | 1s:760 0s:118290\n",
      "ðŸ’¾ Saved | last_row=119099 | 1s:760 0s:118340 | labeled_in_run:1400\n",
      "Labeled 1400/2300 | last_row=119099 | 1s:760 0s:118340\n",
      "Labeled 1450/2300 | last_row=119149 | 1s:760 0s:118390\n",
      "ðŸ’¾ Saved | last_row=119199 | 1s:760 0s:118440 | labeled_in_run:1500\n",
      "Labeled 1500/2300 | last_row=119199 | 1s:760 0s:118440\n",
      "Labeled 1550/2300 | last_row=119249 | 1s:760 0s:118490\n",
      "ðŸ’¾ Saved | last_row=119299 | 1s:760 0s:118540 | labeled_in_run:1600\n",
      "Labeled 1600/2300 | last_row=119299 | 1s:760 0s:118540\n",
      "Labeled 1650/2300 | last_row=119349 | 1s:760 0s:118590\n",
      "ðŸ’¾ Saved | last_row=119399 | 1s:761 0s:118639 | labeled_in_run:1700\n",
      "Labeled 1700/2300 | last_row=119399 | 1s:761 0s:118639\n",
      "Labeled 1750/2300 | last_row=119449 | 1s:761 0s:118689\n",
      "ðŸ’¾ Saved | last_row=119499 | 1s:761 0s:118739 | labeled_in_run:1800\n",
      "Labeled 1800/2300 | last_row=119499 | 1s:761 0s:118739\n",
      "Labeled 1850/2300 | last_row=119549 | 1s:762 0s:118788\n",
      "ðŸ’¾ Saved | last_row=119599 | 1s:763 0s:118837 | labeled_in_run:1900\n",
      "Labeled 1900/2300 | last_row=119599 | 1s:763 0s:118837\n",
      "Labeled 1950/2300 | last_row=119649 | 1s:763 0s:118887\n",
      "ðŸ’¾ Saved | last_row=119699 | 1s:764 0s:118936 | labeled_in_run:2000\n",
      "Labeled 2000/2300 | last_row=119699 | 1s:764 0s:118936\n",
      "Labeled 2050/2300 | last_row=119749 | 1s:764 0s:118986\n",
      "ðŸ’¾ Saved | last_row=119799 | 1s:764 0s:119036 | labeled_in_run:2100\n",
      "Labeled 2100/2300 | last_row=119799 | 1s:764 0s:119036\n",
      "Labeled 2150/2300 | last_row=119849 | 1s:764 0s:119086\n",
      "ðŸ’¾ Saved | last_row=119899 | 1s:764 0s:119136 | labeled_in_run:2200\n",
      "Labeled 2200/2300 | last_row=119899 | 1s:764 0s:119136\n",
      "Labeled 2250/2300 | last_row=119949 | 1s:764 0s:119186\n",
      "ðŸ’¾ Saved | last_row=119999 | 1s:766 0s:119234 | labeled_in_run:2300\n",
      "Labeled 2300/2300 | last_row=119999 | 1s:766 0s:119234\n",
      "âœ… Progress saved: /Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/data/preprocessed_kosovo_news_with_is_sarcasm_v1.csv\n",
      "âœ… Balanced dataset saved: /Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/data/sarcasm_balanced_1500_1500_v1.csv\n",
      "Balanced counts: {0: 1500, 1: 766}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Sarcasm labeling (OpenAI) - RESUME FRIENDLY + CLEAN DATASET\n",
    "# - Reads .env (override=True)\n",
    "# - Keeps ONLY: content, category, (optional source), is_sarcasm, (optional sarcasm_confidence)\n",
    "# - Labels ONLY rows where is_sarcasm is missing\n",
    "# - Saves progress every SAVE_EVERY rows -> can stop/restart anytime\n",
    "# - Builds a balanced dataset at the end\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# -----------------------\n",
    "# ENV (.env)\n",
    "# -----------------------\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4.1-nano\")  # cheap; alt: gpt-4o-mini\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")              # âœ… from .env (NO hardcode)\n",
    "\n",
    "MAX_CHARS = int(os.getenv(\"MAX_CHARS\", \"2000\"))\n",
    "SLEEP_BETWEEN = float(os.getenv(\"SLEEP_BETWEEN\", \"0.15\"))\n",
    "MAX_RETRIES = int(os.getenv(\"MAX_RETRIES\", \"5\"))\n",
    "SAVE_EVERY = int(os.getenv(\"SAVE_EVERY\", \"100\"))\n",
    "\n",
    "TARGET_SARCASM = int(os.getenv(\"TARGET_SARCASM\", \"1500\"))\n",
    "TARGET_NON = int(os.getenv(\"TARGET_NON\", \"1500\"))\n",
    "RANDOM_SEED = int(os.getenv(\"RANDOM_SEED\", \"42\"))\n",
    "\n",
    "KEEP_SOURCE = True          # keep 'source' column or not\n",
    "KEEP_CONFIDENCE = True      # keep 'sarcasm_confidence' column or not\n",
    "\n",
    "# Repo root resolver (works from notebooks/)\n",
    "REPO_ROOT = Path.cwd()\n",
    "while REPO_ROOT != REPO_ROOT.parent and not (REPO_ROOT / \"data\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "INPUT_FILE = DATA_DIR / \"preprocessed_kosovo_news.csv\"\n",
    "PROGRESS_FILE = DATA_DIR / \"preprocessed_kosovo_news_with_is_sarcasm_v1.csv\"\n",
    "FINAL_BALANCED_FILE = DATA_DIR / f\"sarcasm_balanced_{TARGET_SARCASM}_{TARGET_NON}_v1.csv\"\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def truncate_text(s: str, max_chars: int) -> str:\n",
    "    s = str(s or \"\").strip()\n",
    "    if len(s) <= max_chars:\n",
    "        return s\n",
    "    return s[: max_chars - 50] + \"\\n\\n[TRUNCATED]\"\n",
    "\n",
    "def backoff_sleep(attempt: int) -> None:\n",
    "    base = min(2 ** attempt, 20)\n",
    "    jitter = random.uniform(0, 0.5)\n",
    "    time.sleep(base + jitter)\n",
    "\n",
    "def extract_json_loose(text: str) -> dict:\n",
    "    text = (text or \"\").strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        raise ValueError(f\"No JSON found: {text[:200]}\")\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "def read_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "def detect_sarcasm_openai(client: OpenAI, text: str) -> tuple[int, float]:\n",
    "    system = (\n",
    "        \"You are a sarcasm detector for Albanian-language news texts.\\n\"\n",
    "        \"Return ONLY valid JSON in this exact format:\\n\"\n",
    "        '{\"sarcasm\": 0 or 1, \"confidence\": 0.0-1.0}\\n'\n",
    "        \"Rules:\\n\"\n",
    "        \"- sarcasm=1 if there is irony/mockery/sarcasm or meaning is opposite of literal.\\n\"\n",
    "        \"- sarcasm=0 otherwise.\\n\"\n",
    "        \"- If uncertain, choose 0.\\n\"\n",
    "        \"No extra text.\"\n",
    "    )\n",
    "\n",
    "    user_text = truncate_text(text, MAX_CHARS)\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            resp = client.responses.create(\n",
    "                model=MODEL,\n",
    "                input=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": user_text},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_output_tokens=80,\n",
    "            )\n",
    "            out_text = (getattr(resp, \"output_text\", None) or \"\").strip()\n",
    "            data = extract_json_loose(out_text)\n",
    "            sarcasm = int(data.get(\"sarcasm\", 0))\n",
    "            conf = float(data.get(\"confidence\", 0.0))\n",
    "            return sarcasm, conf\n",
    "        except Exception:\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                raise\n",
    "            backoff_sleep(attempt)\n",
    "\n",
    "    return 0, 0.0\n",
    "\n",
    "# -----------------------\n",
    "# MAIN\n",
    "# -----------------------\n",
    "def main():\n",
    "    if not API_KEY:\n",
    "        raise SystemExit(\"Missing OPENAI_API_KEY. Put it in .env and restart kernel.\")\n",
    "\n",
    "    random.seed(RANDOM_SEED)\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "    # 1) Load progress or input\n",
    "    if PROGRESS_FILE.exists():\n",
    "        df = read_csv_robust(PROGRESS_FILE)\n",
    "        print(\"Resuming existing progress file:\", PROGRESS_FILE)\n",
    "    else:\n",
    "        df = read_csv_robust(INPUT_FILE)\n",
    "        print(\"Starting from input file:\", INPUT_FILE)\n",
    "\n",
    "    # 2) Normalize / ensure needed columns\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    if \"content\" not in df.columns:\n",
    "        raise ValueError(\"Column 'content' not found.\")\n",
    "\n",
    "    if \"category\" not in df.columns:\n",
    "        df[\"category\"] = pd.NA\n",
    "\n",
    "    if KEEP_SOURCE and \"source\" not in df.columns:\n",
    "        df[\"source\"] = pd.NA\n",
    "\n",
    "    if \"is_sarcasm\" not in df.columns:\n",
    "        df[\"is_sarcasm\"] = pd.NA\n",
    "\n",
    "    if KEEP_CONFIDENCE and \"sarcasm_confidence\" not in df.columns:\n",
    "        df[\"sarcasm_confidence\"] = pd.NA\n",
    "\n",
    "    # 3) Keep ONLY the small columns (removes title/text/etc permanently)\n",
    "    keep_cols = [\"content\", \"category\", \"is_sarcasm\"]\n",
    "    if KEEP_SOURCE:\n",
    "        keep_cols.append(\"source\")\n",
    "    if KEEP_CONFIDENCE:\n",
    "        keep_cols.append(\"sarcasm_confidence\")\n",
    "\n",
    "    df = df[keep_cols].copy()\n",
    "\n",
    "    # 4) Clean content + normalize labels\n",
    "    df[\"content\"] = df[\"content\"].fillna(\"\").astype(str).str.strip()\n",
    "    df = df[df[\"content\"] != \"\"].reset_index(drop=True)\n",
    "\n",
    "    # normalize existing labels to int where possible\n",
    "    labeled_mask = df[\"is_sarcasm\"].isin([0, 1, \"0\", \"1\", 0.0, 1.0])\n",
    "    df.loc[labeled_mask, \"is_sarcasm\"] = df.loc[labeled_mask, \"is_sarcasm\"].astype(int)\n",
    "\n",
    "    current_ones = int((df[\"is_sarcasm\"] == 1).sum())\n",
    "    current_zeros = int((df[\"is_sarcasm\"] == 0).sum())\n",
    "    print(f\"Already labeled -> 1s:{current_ones}, 0s:{current_zeros}, total:{len(df)}\")\n",
    "\n",
    "    # 5) IMPORTANT: only label rows where is_sarcasm is missing (resume-friendly)\n",
    "    missing_idx = df.index[~df[\"is_sarcasm\"].isin([0, 1])].tolist()\n",
    "    print(\"Rows to label (missing):\", len(missing_idx))\n",
    "\n",
    "    processed_since_save = 0\n",
    "\n",
    "    for n, i in enumerate(missing_idx, start=1):\n",
    "        if current_ones >= TARGET_SARCASM and current_zeros >= TARGET_NON:\n",
    "            print(f\"âœ… Stop: reached targets 1s={TARGET_SARCASM} and 0s={TARGET_NON}\")\n",
    "            break\n",
    "\n",
    "        text = df.at[i, \"content\"]\n",
    "        if not text:\n",
    "            df.at[i, \"is_sarcasm\"] = 0\n",
    "            if KEEP_CONFIDENCE:\n",
    "                df.at[i, \"sarcasm_confidence\"] = 0.0\n",
    "            current_zeros += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            label, conf = detect_sarcasm_openai(client, text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at row {i}: {e} -> set 0\")\n",
    "            label, conf = 0, 0.0\n",
    "\n",
    "        df.at[i, \"is_sarcasm\"] = int(label)\n",
    "        if KEEP_CONFIDENCE:\n",
    "            df.at[i, \"sarcasm_confidence\"] = float(conf)\n",
    "\n",
    "        if label == 1:\n",
    "            current_ones += 1\n",
    "        else:\n",
    "            current_zeros += 1\n",
    "\n",
    "        processed_since_save += 1\n",
    "\n",
    "        if processed_since_save >= SAVE_EVERY:\n",
    "            df.to_csv(PROGRESS_FILE, index=False, encoding=\"utf-8\")\n",
    "            print(f\"ðŸ’¾ Saved | last_row={i} | 1s:{current_ones} 0s:{current_zeros} | labeled_in_run:{n}\")\n",
    "            processed_since_save = 0\n",
    "\n",
    "        if n % 50 == 0:\n",
    "            print(f\"Labeled {n}/{len(missing_idx)} | last_row={i} | 1s:{current_ones} 0s:{current_zeros}\")\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    # final save\n",
    "    df.to_csv(PROGRESS_FILE, index=False, encoding=\"utf-8\")\n",
    "    print(\"âœ… Progress saved:\", PROGRESS_FILE)\n",
    "\n",
    "    # 6) Build balanced dataset (only from labeled rows)\n",
    "    labeled_df = df[df[\"is_sarcasm\"].isin([0, 1])].copy()\n",
    "    labeled_df[\"is_sarcasm\"] = labeled_df[\"is_sarcasm\"].astype(int)\n",
    "    labeled_df = labeled_df.drop_duplicates(subset=[\"content\"]).reset_index(drop=True)\n",
    "\n",
    "    ones = labeled_df[labeled_df[\"is_sarcasm\"] == 1]\n",
    "    zeros = labeled_df[labeled_df[\"is_sarcasm\"] == 0]\n",
    "\n",
    "    n1 = min(TARGET_SARCASM, len(ones))\n",
    "    n0 = min(TARGET_NON, len(zeros))\n",
    "\n",
    "    balanced = pd.concat(\n",
    "        [\n",
    "            ones.sample(n=n1, random_state=RANDOM_SEED),\n",
    "            zeros.sample(n=n0, random_state=RANDOM_SEED),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    ).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    # final columns (what you asked)\n",
    "    final_cols = [\"content\", \"category\", \"is_sarcasm\"]\n",
    "    if KEEP_SOURCE:\n",
    "        final_cols.append(\"source\")\n",
    "    if KEEP_CONFIDENCE:\n",
    "        final_cols.append(\"sarcasm_confidence\")\n",
    "\n",
    "    balanced = balanced[final_cols]\n",
    "    balanced.to_csv(FINAL_BALANCED_FILE, index=False, encoding=\"utf-8\")\n",
    "    print(\"âœ… Balanced dataset saved:\", FINAL_BALANCED_FILE)\n",
    "    print(\"Balanced counts:\", balanced[\"is_sarcasm\"].value_counts().to_dict())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d7827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred counts: {0: 2259, 1: 741}\n",
      "prog counts: {}\n",
      "merged counts: {0: 2259, 1: 741}\n",
      "balanced counts: {0: 1500, 1: 741}\n",
      "Saved balanced dataset: /Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/data/sarcasm_balanced_1500_1500_merged.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------\n",
    "# PATHS\n",
    "# -----------------------\n",
    "REPO_ROOT = Path.cwd()\n",
    "while REPO_ROOT != REPO_ROOT.parent and not (REPO_ROOT / \"data\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "PRED_FILE = DATA_DIR / \"annotation_kungulli_3k_with_sarcasm.csv\"\n",
    "PROGRESS_FILE = DATA_DIR / \"preprocessed_kosovo_news_with_is_sarcasm_v1.csv\"\n",
    "\n",
    "OUT_MERGED_ALL = DATA_DIR / \"sarcasm_merged_all_labeled.csv\"\n",
    "OUT_BALANCED = DATA_DIR / \"sarcasm_balanced_1500_1500_merged.csv\"\n",
    "\n",
    "TARGET_1 = 1500\n",
    "TARGET_0 = 1500\n",
    "SEED = 42\n",
    "\n",
    "# -----------------------\n",
    "# HELPERS\n",
    "# -----------------------\n",
    "def read_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "def normalize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    # content\n",
    "    if \"content\" not in df.columns:\n",
    "        raise ValueError(\"Missing column 'content' in one of the files.\")\n",
    "    df[\"content\"] = df[\"content\"].fillna(\"\").astype(str).str.strip()\n",
    "    df = df[df[\"content\"] != \"\"].copy()\n",
    "\n",
    "    # category (optional)\n",
    "    if \"category\" not in df.columns:\n",
    "        df[\"category\"] = pd.NA\n",
    "\n",
    "    # label column name differences\n",
    "    if \"is_sarcasm\" not in df.columns:\n",
    "        if \"sarcasm\" in df.columns:\n",
    "            df[\"is_sarcasm\"] = df[\"sarcasm\"]\n",
    "        elif \"labels\" in df.columns:\n",
    "            df[\"is_sarcasm\"] = df[\"labels\"]\n",
    "        else:\n",
    "            raise ValueError(\"Missing label column: expected 'is_sarcasm' (or sarcasm/labels).\")\n",
    "\n",
    "    # normalize label values to int 0/1\n",
    "    df[\"is_sarcasm\"] = df[\"is_sarcasm\"].map(lambda x: str(x).strip() if pd.notna(x) else x)\n",
    "    df = df[df[\"is_sarcasm\"].isin([\"0\", \"1\", 0, 1, 0.0, 1.0])].copy()\n",
    "    df[\"is_sarcasm\"] = df[\"is_sarcasm\"].astype(int)\n",
    "\n",
    "    # keep only needed cols\n",
    "    df = df[[\"content\", \"category\", \"is_sarcasm\"]].copy()\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# LOAD\n",
    "# -----------------------\n",
    "df_pred = normalize(read_csv_robust(PRED_FILE))\n",
    "df_prog = normalize(read_csv_robust(PROGRESS_FILE))\n",
    "\n",
    "print(\"pred counts:\", df_pred[\"is_sarcasm\"].value_counts().to_dict())\n",
    "print(\"prog counts:\", df_prog[\"is_sarcasm\"].value_counts().to_dict())\n",
    "\n",
    "# -----------------------\n",
    "# MERGE + DEDUPE\n",
    "# -----------------------\n",
    "merged = pd.concat([df_pred, df_prog], ignore_index=True)\n",
    "\n",
    "# normalize whitespace just in case\n",
    "merged[\"content\"] = merged[\"content\"].astype(str).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "# drop duplicates by content (keeps first)\n",
    "merged = merged.drop_duplicates(subset=[\"content\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"merged counts:\", merged[\"is_sarcasm\"].value_counts().to_dict())\n",
    "\n",
    "# -----------------------\n",
    "# BALANCE (1500/1500)\n",
    "# -----------------------\n",
    "ones = merged[merged[\"is_sarcasm\"] == 1]\n",
    "zeros = merged[merged[\"is_sarcasm\"] == 0]\n",
    "\n",
    "n1 = min(TARGET_1, len(ones))\n",
    "n0 = min(TARGET_0, len(zeros))\n",
    "\n",
    "balanced = pd.concat(\n",
    "    [\n",
    "        ones.sample(n=n1, random_state=SEED),\n",
    "        zeros.sample(n=n0, random_state=SEED),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"balanced counts:\", balanced[\"is_sarcasm\"].value_counts().to_dict())\n",
    "balanced.to_csv(OUT_BALANCED, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved balanced dataset:\", OUT_BALANCED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
