{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd8491c",
   "metadata": {},
   "source": [
    "## Sarcasm Detection â€“ Dataset Preparation & Annotation Workflow\n",
    "\n",
    "This notebook prepares the dataset for sarcasm detection experiments.\n",
    "It includes dataset merging, annotation setup, quality control, and final dataset preparation for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a855f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356b5d7",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de09e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path().resolve().parents[1]\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "PREPROCESSED_DF = DATA_DIR / \"preprocessed_kosovo_news.csv\"\n",
    "PREPARED_FOR_ANNOTATION_DF = DATA_DIR / \"annotation_kungulli_3k.csv\"\n",
    "ANNOTATED_DF = DATA_DIR / \"annotation_kungulli_3k_with_sarcasm.csv\"\n",
    "PREPARED_LABELED_DF = DATA_DIR / \"sarcasm_detection_dataset_v1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4663fb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bleronaidrizi/Sources/Master_Tema_e_Diplomes/Punimi/Sarcasm-Detection-Albanian-News-Dataset/data/preprocessed_kosovo_news.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kur Beatrice Borromeo u martua me Pierre Casir...</td>\n",
       "      <td>As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...</td>\n",
       "      <td>Fun;ArgÃ«tim</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GjatÃ« kontrollit tÃ« automjetit zyrtarÃ«t polico...</td>\n",
       "      <td>I kapen 10 kg substanca narkotike nÃ« BMW X5, a...</td>\n",
       "      <td>Lajme;Nacionale</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>I kapen 10 kg substanca narkotike nÃ« BMW X5, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enca Haxhia njihet si njÃ« ndÃ«r kÃ«ngÃ«taret mÃ« s...</td>\n",
       "      <td>Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GurÃ«t nÃ« veshka janÃ« depozitime minerale qÃ« fo...</td>\n",
       "      <td>GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...</td>\n",
       "      <td>Lifestyle;ShÃ«ndeti</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NÃ« vendim thuhet se Zyra e Prokurorit tÃ« Speci...</td>\n",
       "      <td>Trupi gjykues ua vazhdon paraburgimin Nasim Ha...</td>\n",
       "      <td>Lajme;Nacionale</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>Trupi gjykues ua vazhdon paraburgimin Nasim Ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Kur Beatrice Borromeo u martua me Pierre Casir...   \n",
       "1  GjatÃ« kontrollit tÃ« automjetit zyrtarÃ«t polico...   \n",
       "2  Enca Haxhia njihet si njÃ« ndÃ«r kÃ«ngÃ«taret mÃ« s...   \n",
       "3  GurÃ«t nÃ« veshka janÃ« depozitime minerale qÃ« fo...   \n",
       "4  NÃ« vendim thuhet se Zyra e Prokurorit tÃ« Speci...   \n",
       "\n",
       "                                               title            category  \\\n",
       "0  As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...         Fun;ArgÃ«tim   \n",
       "1  I kapen 10 kg substanca narkotike nÃ« BMW X5, a...     Lajme;Nacionale   \n",
       "2  Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...       Entertainment   \n",
       "3  GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...  Lifestyle;ShÃ«ndeti   \n",
       "4  Trupi gjykues ua vazhdon paraburgimin Nasim Ha...     Lajme;Nacionale   \n",
       "\n",
       "  source                                               text  \n",
       "0  Lajmi  As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...  \n",
       "1  Lajmi  I kapen 10 kg substanca narkotike nÃ« BMW X5, a...  \n",
       "2  Lajmi  Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...  \n",
       "3  Lajmi  GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...  \n",
       "4  Lajmi  Trupi gjykues ua vazhdon paraburgimin Nasim Ha...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(PREPROCESSED_DF)\n",
    "df = pd.read_csv(PREPROCESSED_DF)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d7ecc",
   "metadata": {},
   "source": [
    "### 1. Data Sampling\n",
    "\n",
    "- Load preprocessed dataset\n",
    "- Merge Kungulli sample with other sources\n",
    "- Create final annotation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3c76de5f946aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T17:40:39.172398Z",
     "start_time": "2026-02-19T17:22:19.531739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['content', 'title', 'category', 'source', 'text']\n",
      "Kungulli found: 694\n",
      "Other sources found: 1454473\n",
      "\n",
      "âœ… Saved: /Users/fatlumberisha/Blerona/Sarcasm-Detection-Albanian-News-Dataset/data/annotation_kungulli_3k.csv\n",
      "Total rows: 3000\n",
      "Kungulli in final: 694\n"
     ]
    }
   ],
   "source": [
    "TARGET_N = 3000\n",
    "CHUNKSIZE = 50_000\n",
    "RANDOM_STATE = 42\n",
    "# DEBUG_LIMIT = 4   # None pÃ«r full run\n",
    "\n",
    "SATIRE_DOMAINS = {\"kungulli.com\", \"www.kungulli.com\"}\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = df.columns.astype(str).str.strip().str.lower()\n",
    "    return df\n",
    "\n",
    "def get_output_path(data_path: str, out_file: str) -> str:\n",
    "    folder = os.path.dirname(os.path.abspath(data_path))\n",
    "    return os.path.join(folder, out_file)\n",
    "\n",
    "def extract_domain(url: str) -> str:\n",
    "    if pd.isna(url):\n",
    "        return \"\"\n",
    "    s = str(url).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    if not re.match(r\"^https?://\", s, flags=re.IGNORECASE):\n",
    "        s = \"https://\" + s\n",
    "    try:\n",
    "        host = urlparse(s).netloc.lower()\n",
    "        host = host.split(\":\")[0]\n",
    "        return host\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def read_chunks(path: str):\n",
    "    try:\n",
    "        return pd.read_csv(\n",
    "            path,\n",
    "            chunksize=CHUNKSIZE,\n",
    "            sep=\",\",\n",
    "            engine=\"c\",\n",
    "            on_bad_lines=\"skip\",\n",
    "        )\n",
    "    except Exception:\n",
    "        return pd.read_csv(\n",
    "            path,\n",
    "            chunksize=CHUNKSIZE,\n",
    "            sep=\",\",\n",
    "            engine=\"python\",\n",
    "            on_bad_lines=\"skip\",\n",
    "        )\n",
    "\n",
    "def ensure_source_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    if \"source\" in cols:\n",
    "        df[\"source\"] = df[\"source\"].astype(str).str.strip().str.lower()\n",
    "        return df\n",
    "\n",
    "    if \"url\" in cols:\n",
    "        df[\"source\"] = df[\"url\"].apply(extract_domain)\n",
    "        return df\n",
    "\n",
    "    raise ValueError(\n",
    "        \"The dataset has neither 'source' nor 'url'. I can't find pumpkin without either of them.\"\n",
    "    )\n",
    "\n",
    "def filter_kungulli(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = ensure_source_column(df)\n",
    "    df[\"source\"] = df[\"source\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    mask = df[\"source\"].isin(SATIRE_DOMAINS) | df[\"source\"].str.contains(\"kungulli\", na=False)\n",
    "    return df[mask].copy()\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def run():\n",
    "    df0 = pd.read_csv(PREPROCESSED_DF, nrows=1)\n",
    "    df0 = normalize_cols(df0)\n",
    "    print(\"Header:\", list(df0.columns))\n",
    "\n",
    "    kungulli_rows = []\n",
    "    other_rows = []\n",
    "\n",
    "    for chunk in read_chunks(PREPROCESSED_DF):\n",
    "        chunk = normalize_cols(chunk)\n",
    "\n",
    "        try:\n",
    "            k = filter_kungulli(chunk)\n",
    "        except ValueError as e:\n",
    "            print(\"âŒ\", str(e))\n",
    "            return\n",
    "\n",
    "        # kungulli\n",
    "        if len(k) > 0:\n",
    "            kungulli_rows.append(k)\n",
    "\n",
    "        # not kungulli\n",
    "        chunk = ensure_source_column(chunk)\n",
    "        chunk[\"source\"] = chunk[\"source\"].astype(str).str.lower().str.strip()\n",
    "        mask_other = ~(\n",
    "            chunk[\"source\"].isin(SATIRE_DOMAINS) |\n",
    "            chunk[\"source\"].str.contains(\"kungulli\", na=False)\n",
    "        )\n",
    "        o = chunk[mask_other]\n",
    "        if len(o) > 0:\n",
    "            other_rows.append(o)\n",
    "\n",
    "    if not kungulli_rows:\n",
    "        print(\"No kungulli found.\")\n",
    "        return\n",
    "\n",
    "    df_k = pd.concat(kungulli_rows, ignore_index=True).drop_duplicates()\n",
    "    df_o = pd.concat(other_rows, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    print(\"Kungulli found:\", len(df_k))\n",
    "    print(\"Other sources found:\", len(df_o))\n",
    "\n",
    "    # -------------------------\n",
    "    # Build 3K total\n",
    "    # -------------------------\n",
    "    needed_from_other = TARGET_N - len(df_k)\n",
    "\n",
    "    if needed_from_other <= 0:\n",
    "        df_out = df_k.sample(n=TARGET_N, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        df_o_sample = df_o.sample(\n",
    "            n=min(needed_from_other, len(df_o)),\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        df_out = pd.concat([df_k, df_o_sample], ignore_index=True)\n",
    "\n",
    "    # Shuffle final\n",
    "    df_out = df_out.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "    df_out[\"is_sarcasm(1|0|?)\"] = \"\"\n",
    "\n",
    "    out_path = get_output_path(PREPROCESSED_DF, PREPARED_FOR_ANNOTATION_DF)\n",
    "    df_out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\nâœ… Saved:\", out_path)\n",
    "    print(\"Total rows:\", len(df_out))\n",
    "    print(\"Kungulli in final:\", sum(df_out[\"source\"].str.contains(\"kungulli\")))\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88781e8164bb92e6",
   "metadata": {},
   "source": [
    "### 2. Claude-Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69b31444ca97eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T20:11:05.091828Z",
     "start_time": "2026-02-19T18:17:49.865107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_CSV: /Users/fatlumberisha/Blerona/Sarcasm-Detection-Albanian-News-Dataset/data/annotation_kungulli_3k.csv\n",
      "OUTPUT_CSV: /Users/fatlumberisha/Blerona/Sarcasm-Detection-Albanian-News-Dataset/data/annotation_kungulli_3k_with_sarcasm.csv\n",
      "Processed 0/3000\n",
      "Processed 50/3000\n",
      "ðŸ’¾ Saved progress at row 99\n",
      "Processed 100/3000\n",
      "Processed 150/3000\n",
      "ðŸ’¾ Saved progress at row 199\n",
      "Processed 200/3000\n",
      "Processed 250/3000\n",
      "ðŸ’¾ Saved progress at row 299\n",
      "Processed 300/3000\n",
      "Processed 350/3000\n",
      "ðŸ’¾ Saved progress at row 399\n",
      "Processed 400/3000\n",
      "Processed 450/3000\n",
      "ðŸ’¾ Saved progress at row 499\n",
      "Processed 500/3000\n",
      "Processed 550/3000\n",
      "ðŸ’¾ Saved progress at row 599\n",
      "Processed 600/3000\n",
      "Processed 650/3000\n",
      "ðŸ’¾ Saved progress at row 699\n",
      "Processed 700/3000\n",
      "âš ï¸ Error at row 702: No valid JSON found in model output: ...\n",
      "Processed 750/3000\n",
      "ðŸ’¾ Saved progress at row 800\n",
      "Processed 800/3000\n",
      "Processed 850/3000\n",
      "ðŸ’¾ Saved progress at row 900\n",
      "Processed 900/3000\n",
      "Processed 950/3000\n",
      "ðŸ’¾ Saved progress at row 1000\n",
      "Processed 1000/3000\n",
      "Processed 1050/3000\n",
      "ðŸ’¾ Saved progress at row 1100\n",
      "Processed 1100/3000\n",
      "Processed 1150/3000\n",
      "ðŸ’¾ Saved progress at row 1200\n",
      "Processed 1200/3000\n",
      "Processed 1250/3000\n",
      "âš ï¸ Error at row 1272: No valid JSON found in model output: ...\n",
      "Processed 1300/3000\n",
      "ðŸ’¾ Saved progress at row 1301\n",
      "Processed 1350/3000\n",
      "Processed 1400/3000\n",
      "ðŸ’¾ Saved progress at row 1401\n",
      "Processed 1450/3000\n",
      "Processed 1500/3000\n",
      "ðŸ’¾ Saved progress at row 1501\n",
      "Processed 1550/3000\n",
      "Processed 1600/3000\n",
      "ðŸ’¾ Saved progress at row 1601\n",
      "Processed 1650/3000\n",
      "Processed 1700/3000\n",
      "ðŸ’¾ Saved progress at row 1701\n",
      "Processed 1750/3000\n",
      "Processed 1800/3000\n",
      "ðŸ’¾ Saved progress at row 1801\n",
      "Processed 1850/3000\n",
      "Processed 1900/3000\n",
      "ðŸ’¾ Saved progress at row 1901\n",
      "Processed 1950/3000\n",
      "Processed 2000/3000\n",
      "ðŸ’¾ Saved progress at row 2001\n",
      "Processed 2050/3000\n",
      "Processed 2100/3000\n",
      "ðŸ’¾ Saved progress at row 2101\n",
      "Processed 2150/3000\n",
      "Processed 2200/3000\n",
      "ðŸ’¾ Saved progress at row 2201\n",
      "Processed 2250/3000\n",
      "Processed 2300/3000\n",
      "ðŸ’¾ Saved progress at row 2301\n",
      "Processed 2350/3000\n",
      "âš ï¸ Error at row 2358: No valid JSON found in model output: ...\n",
      "Processed 2400/3000\n",
      "ðŸ’¾ Saved progress at row 2402\n",
      "Processed 2450/3000\n",
      "Processed 2500/3000\n",
      "ðŸ’¾ Saved progress at row 2502\n",
      "Processed 2550/3000\n",
      "Processed 2600/3000\n",
      "ðŸ’¾ Saved progress at row 2602\n",
      "Processed 2650/3000\n",
      "Processed 2700/3000\n",
      "ðŸ’¾ Saved progress at row 2702\n",
      "Processed 2750/3000\n",
      "Processed 2800/3000\n",
      "ðŸ’¾ Saved progress at row 2802\n",
      "Processed 2850/3000\n",
      "Processed 2900/3000\n",
      "ðŸ’¾ Saved progress at row 2902\n",
      "Processed 2950/3000\n",
      "âœ… Done. All rows processed.\n"
     ]
    }
   ],
   "source": [
    "MODEL = os.getenv(\"CLAUDE_MODEL\", \"claude-opus-4-6\")\n",
    "API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "MAX_CHARS = int(os.getenv(\"MAX_CHARS\", \"2500\"))\n",
    "SLEEP_BETWEEN = float(os.getenv(\"SLEEP_BETWEEN\", \"0.1\"))\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def truncate_text(s: str, max_chars: int) -> str:\n",
    "    s = str(s).strip()\n",
    "    if len(s) <= max_chars:\n",
    "        return s\n",
    "    return s[: max_chars - 50] + \"\\n\\n[TRUNCATED]\"\n",
    "\n",
    "def backoff_sleep(attempt: int) -> None:\n",
    "    base = min(2 ** attempt, 20)\n",
    "    jitter = random.uniform(0, 0.5)\n",
    "    time.sleep(base + jitter)\n",
    "\n",
    "def extract_json(text: str) -> dict:\n",
    "    text = (text or \"\").strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return json.loads(text[start:end + 1])\n",
    "\n",
    "    raise ValueError(f\"No valid JSON found in model output: {text[:200]}...\")\n",
    "\n",
    "def read_dataset(path: Path) -> pd.DataFrame:\n",
    "    # robust encoding fallback (du hattest UTF-8 Probleme)\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "def detect_sarcasm(client: anthropic.Anthropic, text: str) -> int:\n",
    "    system = (\n",
    "        \"You are a sarcasm detector for Albanian-language news texts.\\n\"\n",
    "        \"Label sarcasm as 1 if the text uses irony, mockery, sarcasm, or clearly says the opposite of intent.\\n\"\n",
    "        \"Otherwise label 0.\\n\"\n",
    "        \"If uncertain, choose 0.\\n\"\n",
    "        \"Return ONLY valid JSON exactly in this format:\\n\"\n",
    "        '{\"sarcasm\": 0 or 1, \"confidence\": 0.0-1.0}\\n'\n",
    "        \"No extra text.\"\n",
    "    )\n",
    "\n",
    "    user_text = truncate_text(text, MAX_CHARS)\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            msg = client.messages.create(\n",
    "                model=MODEL,\n",
    "                max_tokens=80,\n",
    "                temperature=0.0,\n",
    "                system=system,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_text}],\n",
    "            )\n",
    "            # msg.content ist Liste von Content-BlÃ¶cken; typischerweise 1 Textblock\n",
    "            out_text = \"\".join(block.text for block in msg.content if getattr(block, \"type\", \"\") == \"text\")\n",
    "            data = extract_json(out_text)\n",
    "            return int(data.get(\"sarcasm\", 0))\n",
    "        except Exception:\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                raise\n",
    "            backoff_sleep(attempt)\n",
    "\n",
    "    return 0\n",
    "\n",
    "def main():\n",
    "    if not API_KEY:\n",
    "        raise SystemExit(\"Missing ANTHROPIC_API_KEY in .env\")\n",
    "\n",
    "    print(\"INPUT_CSV:\", PREPARED_FOR_ANNOTATION_DF)\n",
    "    print(\"OUTPUT_CSV:\", ANNOTATED_DF)\n",
    "\n",
    "    df = read_dataset(PREPARED_FOR_ANNOTATION_DF)\n",
    "\n",
    "    if \"content\" not in df.columns:\n",
    "        raise ValueError(\"The column 'content' does not exist.\")\n",
    "\n",
    "    # Create sarcasm column if missing\n",
    "    if \"sarcasm\" not in df.columns:\n",
    "        df[\"sarcasm\"] = None\n",
    "\n",
    "    client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "    total = len(df)\n",
    "    processed_since_save = 0\n",
    "\n",
    "    for i in range(total):\n",
    "        # Skip if already processed\n",
    "        if pd.notna(df.loc[i, \"sarcasm\"]):\n",
    "            continue\n",
    "\n",
    "        text = str(df.loc[i, \"content\"]).strip()\n",
    "        if not text:\n",
    "            df.loc[i, \"sarcasm\"] = 0\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            label = detect_sarcasm(client, text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at row {i}: {e}\")\n",
    "            df.loc[i, \"sarcasm\"] = 0\n",
    "            continue\n",
    "\n",
    "        df.loc[i, \"sarcasm\"] = label\n",
    "        processed_since_save += 1\n",
    "\n",
    "        # Save every 100 rows\n",
    "        if processed_since_save >= 100:\n",
    "            df.to_csv(ANNOTATED_DF, index=False, encoding=\"utf-8\")\n",
    "            print(f\"ðŸ’¾ Saved progress at row {i}\")\n",
    "            processed_since_save = 0\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processed {i}/{total}\")\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    # Final save\n",
    "    df.to_csv(ANNOTATED_DF, index=False, encoding=\"utf-8\")\n",
    "    print(\"âœ… Done. All rows processed.\")\n",
    "\n",
    "main()\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "for m in client.models.list():\n",
    "    print(m.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
