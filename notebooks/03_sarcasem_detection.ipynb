{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40109a66",
   "metadata": {},
   "source": [
    "# Sarcasm Detection in Albanian News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd8491c",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "- Develop a machine learning model (BERT-based) to detect sarcasm in Albanian news articles.\n",
    "- Perform binary classification:  \n",
    "  **Sarcastic (1)** vs **Not Sarcastic (0)**.\n",
    "\n",
    "---\n",
    "\n",
    "--- Challenges\n",
    "\n",
    "- No pre-annotated sarcasm labels exist for Albanian news.\n",
    "- Sarcasm detection requires contextual and semantic understanding.\n",
    "- The dataset is large (~4GB), requiring efficient sampling and preprocessing.\n",
    "- Sarcasm is naturally rare and may lead to class imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "--- Approach\n",
    "\n",
    "--- 1. Data Sampling\n",
    "\n",
    "- Extract a manageable subset (1,500â€“3,000 articles) for manual annotation.\n",
    "- Apply:\n",
    "  - **Stratified sampling** across categories and sources.\n",
    "  - **Keyword-based filtering** to identify potential sarcasm candidates.\n",
    "  - Include articles from satire domains (e.g., Kungulli) as sarcasm candidates.\n",
    "\n",
    "---\n",
    "\n",
    "--- 2. Annotation Process\n",
    "\n",
    "- Two annotators manually label the selected articles.\n",
    "- Labels:\n",
    "  - `1 = Sarcastic`\n",
    "  - `0 = Not Sarcastic`\n",
    "  - `? = Unsure` (for later review)\n",
    "\n",
    "- Create clear annotation guidelines to ensure consistency.\n",
    "- Perform initial calibration:\n",
    "  - Both annotators label the same 100 samples.\n",
    "  - Compare results and refine guidelines.\n",
    "- Resolve disagreements through discussion.\n",
    "\n",
    "---\n",
    "\n",
    "--- 3. Active Learning (Optional Optimization)\n",
    "\n",
    "- Train a preliminary classifier on early labeled data.\n",
    "- Identify uncertain samples (probability close to 0.5).\n",
    "- Prioritize these samples for annotation.\n",
    "- Iteratively improve dataset quality and model performance.\n",
    "\n",
    "---\n",
    "\n",
    "--- 4. Model Training\n",
    "\n",
    "- Fine-tune a multilingual transformer model:\n",
    "  - **XLM-R**\n",
    "  - or **Multilingual BERT**\n",
    "\n",
    "- Compare against baseline models:\n",
    "  - Logistic Regression\n",
    "  - LinearSVC\n",
    "  - Multinomial Naive Bayes\n",
    "\n",
    "- Use standard NLP preprocessing and tokenization.\n",
    "\n",
    "---\n",
    "\n",
    "--- 5. Evaluation Strategy\n",
    "\n",
    "- Split dataset into:\n",
    "  - 70% Training\n",
    "  - 15% Validation\n",
    "  - 15% Test (held-out set)\n",
    "\n",
    "- Apply stratified splitting to maintain class balance.\n",
    "- Avoid data leakage.\n",
    "- Perform cross-validation during development.\n",
    "\n",
    "- Evaluate using:\n",
    "  - **Precision**\n",
    "  - **Recall**\n",
    "  - **F1-score (Primary Metric)**\n",
    "  - Confusion Matrix\n",
    "  - Accuracy\n",
    "\n",
    "---\n",
    "\n",
    "--- Expected Outcome\n",
    "\n",
    "- A trained sarcasm detection model for Albanian news.\n",
    "- The first manually annotated sarcasm dataset in Albanian news domain.\n",
    "- Performance comparison between:\n",
    "  - Classical machine learning models\n",
    "  - Transformer-based deep learning models\n",
    "- A reproducible research pipeline for future sarcasm detection studies.\n",
    "\n",
    "---\n",
    "\n",
    "--- Project Summary\n",
    "\n",
    "This project aims to build the first sarcasm detection system for Albanian news articles by constructing a manually annotated dataset and applying transformer-based classification methods. The study evaluates both classical machine learning approaches and deep learning architectures to determine the most effective method for detecting sarcasm in low-resource languages."
   ]
  },
  {
   "cell_type": "code",
   "id": "631c5041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T09:31:23.586566Z",
     "start_time": "2026-02-16T09:31:23.472398Z"
    }
   },
   "source": [
    "# Used Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "863742b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T09:31:24.718725Z",
     "start_time": "2026-02-16T09:31:24.697042Z"
    }
   },
   "source": [
    "# Helper methods\n",
    "\n",
    "def print_dataset(text, df):\n",
    "    print(\"\\n\" + text + \":\")\n",
    "    display(df.head())\n",
    "\n",
    "def read_dataset(path):\n",
    "    return pd.read_csv(path)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "49f3d950",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "id": "d45ee17c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T09:31:26.742575Z",
     "start_time": "2026-02-16T09:31:26.717686Z"
    }
   },
   "source": [
    "DF_COLUMNS = ['content', 'date', 'title', 'category', 'author', 'source']\n",
    "DF_PATH = \"../data/kosovo_news.csv\"\n",
    "PREPROCESSED_DF_PATH = \"../data/preprocessed_kosovo_news.csv\"\n",
    "SCFA_OUT_FILE  = \"sarcasm_candidates_for_annotation.csv\"\n",
    "TITLE_COL    = \"title\"\n",
    "TEXT_COL     = \"text\"\n",
    "CATEGORY_COL = \"category\"\n",
    "SOURCE_COL   = \"source\""
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "d23ae99e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T09:34:47.557389Z",
     "start_time": "2026-02-16T09:31:28.356073Z"
    }
   },
   "source": [
    "df = read_dataset(PREPROCESSED_DF_PATH)\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                             content  \\\n",
       "0  Kur Beatrice Borromeo u martua me Pierre Casir...   \n",
       "1  GjatÃ« kontrollit tÃ« automjetit zyrtarÃ«t polico...   \n",
       "2  Enca Haxhia njihet si njÃ« ndÃ«r kÃ«ngÃ«taret mÃ« s...   \n",
       "3  GurÃ«t nÃ« veshka janÃ« depozitime minerale qÃ« fo...   \n",
       "4  NÃ« vendim thuhet se Zyra e Prokurorit tÃ« Speci...   \n",
       "\n",
       "                                               title            category  \\\n",
       "0  As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...         Fun;ArgÃ«tim   \n",
       "1  I kapen 10 kg substanca narkotike nÃ« BMW X5, a...     Lajme;Nacionale   \n",
       "2  Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...       Entertainment   \n",
       "3  GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...  Lifestyle;ShÃ«ndeti   \n",
       "4  Trupi gjykues ua vazhdon paraburgimin Nasim Ha...     Lajme;Nacionale   \n",
       "\n",
       "  source                                               text  \n",
       "0  Lajmi  As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...  \n",
       "1  Lajmi  I kapen 10 kg substanca narkotike nÃ« BMW X5, a...  \n",
       "2  Lajmi  Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...  \n",
       "3  Lajmi  GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...  \n",
       "4  Lajmi  Trupi gjykues ua vazhdon paraburgimin Nasim Ha...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kur Beatrice Borromeo u martua me Pierre Casir...</td>\n",
       "      <td>As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...</td>\n",
       "      <td>Fun;ArgÃ«tim</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>As Kate as Meghan; ja cila Ã«shtÃ« princesha mÃ« ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GjatÃ« kontrollit tÃ« automjetit zyrtarÃ«t polico...</td>\n",
       "      <td>I kapen 10 kg substanca narkotike nÃ« BMW X5, a...</td>\n",
       "      <td>Lajme;Nacionale</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>I kapen 10 kg substanca narkotike nÃ« BMW X5, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enca Haxhia njihet si njÃ« ndÃ«r kÃ«ngÃ«taret mÃ« s...</td>\n",
       "      <td>Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>Enca e quan jetÃ« pushimin nÃ« plazh me poza nÃ« ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GurÃ«t nÃ« veshka janÃ« depozitime minerale qÃ« fo...</td>\n",
       "      <td>GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...</td>\n",
       "      <td>Lifestyle;ShÃ«ndeti</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>GurÃ«t nÃ« veshka â€“ Kurat natyrale dhe si tâ€™i pÃ«...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NÃ« vendim thuhet se Zyra e Prokurorit tÃ« Speci...</td>\n",
       "      <td>Trupi gjykues ua vazhdon paraburgimin Nasim Ha...</td>\n",
       "      <td>Lajme;Nacionale</td>\n",
       "      <td>Lajmi</td>\n",
       "      <td>Trupi gjykues ua vazhdon paraburgimin Nasim Ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "e00d79b7",
   "metadata": {},
   "source": [
    "\n",
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d7ecc",
   "metadata": {},
   "source": [
    "### 1. Data Sampling"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T17:40:39.172398Z",
     "start_time": "2026-02-19T17:22:19.531739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "RAW_DATA_PATH = \"../data/preprocessed_kosovo_news.csv\"      # <-- vendos path-in e kosovo_news (RAW)\n",
    "OUT_FILE = \"annotation_kungulli_3k.csv\"\n",
    "\n",
    "TARGET_N = 3000\n",
    "CHUNKSIZE = 50_000\n",
    "RANDOM_STATE = 42\n",
    "DEBUG_LIMIT = 4   # None pÃ«r full run\n",
    "\n",
    "# Mundesh me shtu variante nÃ«se i gjen me/pa www\n",
    "SATIRE_DOMAINS = {\"kungulli.com\", \"www.kungulli.com\"}\n",
    "\n",
    "# âœ… Fix for: Error: field larger than field limit (131072)\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = df.columns.astype(str).str.strip().str.lower()\n",
    "    return df\n",
    "\n",
    "def get_output_path(data_path: str, out_file: str) -> str:\n",
    "    folder = os.path.dirname(os.path.abspath(data_path))\n",
    "    return os.path.join(folder, out_file)\n",
    "\n",
    "def extract_domain(url: str) -> str:\n",
    "    if pd.isna(url):\n",
    "        return \"\"\n",
    "    s = str(url).strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # nÃ«se url nuk ka skemÃ«, ia shtojmÃ« pÃ«r urlparse\n",
    "    if not re.match(r\"^https?://\", s, flags=re.IGNORECASE):\n",
    "        s = \"https://\" + s\n",
    "    try:\n",
    "        host = urlparse(s).netloc.lower()\n",
    "        # hiq portat p.sh. :443\n",
    "        host = host.split(\":\")[0]\n",
    "        return host\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def read_chunks(path: str):\n",
    "    \"\"\"\n",
    "    Try fast C engine first; fallback to python if needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(\n",
    "            path,\n",
    "            chunksize=CHUNKSIZE,\n",
    "            sep=\",\",\n",
    "            engine=\"c\",\n",
    "            on_bad_lines=\"skip\",\n",
    "        )\n",
    "    except Exception:\n",
    "        return pd.read_csv(\n",
    "            path,\n",
    "            chunksize=CHUNKSIZE,\n",
    "            sep=\",\",\n",
    "            engine=\"python\",\n",
    "            on_bad_lines=\"skip\",\n",
    "        )\n",
    "\n",
    "def ensure_source_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    NÃ«se ka 'source' e pÃ«rdor.\n",
    "    NÃ«se sâ€™ka 'source' por ka 'url', e nxjerr domain-in si 'source'.\n",
    "    \"\"\"\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    if \"source\" in cols:\n",
    "        df[\"source\"] = df[\"source\"].astype(str).str.strip().str.lower()\n",
    "        return df\n",
    "\n",
    "    if \"url\" in cols:\n",
    "        df[\"source\"] = df[\"url\"].apply(extract_domain)\n",
    "        return df\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Dataset-i nuk ka as 'source' as 'url'. Sâ€™po mundem me gjet kungulli pa njÃ«rÃ«n prej tyre.\"\n",
    "    )\n",
    "\n",
    "def filter_kungulli(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = ensure_source_column(df)\n",
    "    df[\"source\"] = df[\"source\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # pranon edhe raste ku source Ã«shtÃ« mÃ« i gjatÃ« (p.sh. 'https://www.kungulli.com/...')\n",
    "    mask = df[\"source\"].isin(SATIRE_DOMAINS) | df[\"source\"].str.contains(\"kungulli\", na=False)\n",
    "    return df[mask].copy()\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def run():\n",
    "    df0 = pd.read_csv(RAW_DATA_PATH, nrows=1)\n",
    "    df0 = normalize_cols(df0)\n",
    "    print(\"Header:\", list(df0.columns))\n",
    "\n",
    "    kungulli_rows = []\n",
    "    other_rows = []\n",
    "\n",
    "    for chunk in read_chunks(RAW_DATA_PATH):\n",
    "        chunk = normalize_cols(chunk)\n",
    "\n",
    "        try:\n",
    "            k = filter_kungulli(chunk)\n",
    "        except ValueError as e:\n",
    "            print(\"âŒ\", str(e))\n",
    "            return\n",
    "\n",
    "        # kungulli\n",
    "        if len(k) > 0:\n",
    "            kungulli_rows.append(k)\n",
    "\n",
    "        # jo kungulli\n",
    "        chunk = ensure_source_column(chunk)\n",
    "        chunk[\"source\"] = chunk[\"source\"].astype(str).str.lower().str.strip()\n",
    "        mask_other = ~(\n",
    "            chunk[\"source\"].isin(SATIRE_DOMAINS) |\n",
    "            chunk[\"source\"].str.contains(\"kungulli\", na=False)\n",
    "        )\n",
    "        o = chunk[mask_other]\n",
    "        if len(o) > 0:\n",
    "            other_rows.append(o)\n",
    "\n",
    "    if not kungulli_rows:\n",
    "        print(\"âŒ Sâ€™u gjet asnjÃ« kungulli.\")\n",
    "        return\n",
    "\n",
    "    df_k = pd.concat(kungulli_rows, ignore_index=True).drop_duplicates()\n",
    "    df_o = pd.concat(other_rows, ignore_index=True).drop_duplicates()\n",
    "\n",
    "    print(\"Kungulli found:\", len(df_k))\n",
    "    print(\"Other sources found:\", len(df_o))\n",
    "\n",
    "    # -------------------------\n",
    "    # Build 3K total\n",
    "    # -------------------------\n",
    "    needed_from_other = TARGET_N - len(df_k)\n",
    "\n",
    "    if needed_from_other <= 0:\n",
    "        df_out = df_k.sample(n=TARGET_N, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        df_o_sample = df_o.sample(\n",
    "            n=min(needed_from_other, len(df_o)),\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        df_out = pd.concat([df_k, df_o_sample], ignore_index=True)\n",
    "\n",
    "    # Shuffle final\n",
    "    df_out = df_out.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "    df_out[\"is_sarcasm(1|0|?)\"] = \"\"\n",
    "\n",
    "    out_path = get_output_path(RAW_DATA_PATH, OUT_FILE)\n",
    "    df_out.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\nâœ… Saved:\", out_path)\n",
    "    print(\"Total rows:\", len(df_out))\n",
    "    print(\"Kungulli in final:\", sum(df_out[\"source\"].str.contains(\"kungulli\")))\n",
    "\n",
    "run()"
   ],
   "id": "15b3c76de5f946aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['content', 'title', 'category', 'source', 'text']\n",
      "Kungulli found: 694\n",
      "Other sources found: 1454473\n",
      "\n",
      "âœ… Saved: /Users/fatlumberisha/Blerona/Sarcasm-Detection-Albanian-News-Dataset/data/annotation_kungulli_3k.csv\n",
      "Total rows: 3000\n",
      "Kungulli in final: 694\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Claude-Version",
   "id": "88781e8164bb92e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T20:11:05.091828Z",
     "start_time": "2026-02-19T18:17:49.865107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "BASE_DIR = Path(os.getcwd())          # Notebook-safe; fÃ¼r .py kannst du __file__ nehmen\n",
    "INPUT_CSV = BASE_DIR.parent / \"data\" / \"annotation_kungulli_3k.csv\"\n",
    "OUTPUT_CSV = BASE_DIR.parent / \"data\" / \"annotation_kungulli_3k_with_sarcasm.csv\"\n",
    "\n",
    "MODEL = os.getenv(\"CLAUDE_MODEL\", \"claude-opus-4-6\")\n",
    "API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "MAX_CHARS = int(os.getenv(\"MAX_CHARS\", \"2500\"))\n",
    "SLEEP_BETWEEN = float(os.getenv(\"SLEEP_BETWEEN\", \"0.1\"))\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def truncate_text(s: str, max_chars: int) -> str:\n",
    "    s = str(s).strip()\n",
    "    if len(s) <= max_chars:\n",
    "        return s\n",
    "    return s[: max_chars - 50] + \"\\n\\n[TRUNCATED]\"\n",
    "\n",
    "def backoff_sleep(attempt: int) -> None:\n",
    "    base = min(2 ** attempt, 20)\n",
    "    jitter = random.uniform(0, 0.5)\n",
    "    time.sleep(base + jitter)\n",
    "\n",
    "def extract_json(text: str) -> dict:\n",
    "    text = (text or \"\").strip()\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return json.loads(text[start:end + 1])\n",
    "\n",
    "    raise ValueError(f\"No valid JSON found in model output: {text[:200]}...\")\n",
    "\n",
    "def read_dataset(path: Path) -> pd.DataFrame:\n",
    "    # robust encoding fallback (du hattest UTF-8 Probleme)\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\", encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "def detect_sarcasm(client: anthropic.Anthropic, text: str) -> int:\n",
    "    system = (\n",
    "        \"You are a sarcasm detector for Albanian-language news texts.\\n\"\n",
    "        \"Label sarcasm as 1 if the text uses irony, mockery, sarcasm, or clearly says the opposite of intent.\\n\"\n",
    "        \"Otherwise label 0.\\n\"\n",
    "        \"If uncertain, choose 0.\\n\"\n",
    "        \"Return ONLY valid JSON exactly in this format:\\n\"\n",
    "        '{\"sarcasm\": 0 or 1, \"confidence\": 0.0-1.0}\\n'\n",
    "        \"No extra text.\"\n",
    "    )\n",
    "\n",
    "    user_text = truncate_text(text, MAX_CHARS)\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            msg = client.messages.create(\n",
    "                model=MODEL,\n",
    "                max_tokens=80,\n",
    "                temperature=0.0,\n",
    "                system=system,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_text}],\n",
    "            )\n",
    "            # msg.content ist Liste von Content-BlÃ¶cken; typischerweise 1 Textblock\n",
    "            out_text = \"\".join(block.text for block in msg.content if getattr(block, \"type\", \"\") == \"text\")\n",
    "            data = extract_json(out_text)\n",
    "            return int(data.get(\"sarcasm\", 0))\n",
    "        except Exception:\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                raise\n",
    "            backoff_sleep(attempt)\n",
    "\n",
    "    return 0\n",
    "\n",
    "def main():\n",
    "    if not API_KEY:\n",
    "        raise SystemExit(\"Missing ANTHROPIC_API_KEY in .env\")\n",
    "\n",
    "    print(\"INPUT_CSV:\", INPUT_CSV)\n",
    "    print(\"OUTPUT_CSV:\", OUTPUT_CSV)\n",
    "\n",
    "    df = read_dataset(INPUT_CSV)\n",
    "\n",
    "    if \"content\" not in df.columns:\n",
    "        raise ValueError(\"Kolona 'content' nuk ekziston.\")\n",
    "\n",
    "    # Create sarcasm column if missing\n",
    "    if \"sarcasm\" not in df.columns:\n",
    "        df[\"sarcasm\"] = None\n",
    "\n",
    "    client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "    total = len(df)\n",
    "    processed_since_save = 0\n",
    "\n",
    "    for i in range(total):\n",
    "        # Skip if already processed\n",
    "        if pd.notna(df.loc[i, \"sarcasm\"]):\n",
    "            continue\n",
    "\n",
    "        text = str(df.loc[i, \"content\"]).strip()\n",
    "        if not text:\n",
    "            df.loc[i, \"sarcasm\"] = 0\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            label = detect_sarcasm(client, text)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error at row {i}: {e}\")\n",
    "            df.loc[i, \"sarcasm\"] = 0\n",
    "            continue\n",
    "\n",
    "        df.loc[i, \"sarcasm\"] = label\n",
    "        processed_since_save += 1\n",
    "\n",
    "        # Save every 100 rows\n",
    "        if processed_since_save >= 100:\n",
    "            df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "            print(f\"ğŸ’¾ Saved progress at row {i}\")\n",
    "            processed_since_save = 0\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processed {i}/{total}\")\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    # Final save\n",
    "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(\"âœ… Done. All rows processed.\")\n",
    "\n",
    "main()"
   ],
   "id": "fa69b31444ca97eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_CSV: /Users/fatlumberisha/Blerona/Sarcasm-Detection-Albanian-News-Dataset/data/annotation_kungulli_3k.csv\n",
      "OUTPUT_CSV: /Users/fatlumberisha/Blerona/Sarcasm-Detection-Albanian-News-Dataset/data/annotation_kungulli_3k_with_sarcasm.csv\n",
      "Processed 0/3000\n",
      "Processed 50/3000\n",
      "ğŸ’¾ Saved progress at row 99\n",
      "Processed 100/3000\n",
      "Processed 150/3000\n",
      "ğŸ’¾ Saved progress at row 199\n",
      "Processed 200/3000\n",
      "Processed 250/3000\n",
      "ğŸ’¾ Saved progress at row 299\n",
      "Processed 300/3000\n",
      "Processed 350/3000\n",
      "ğŸ’¾ Saved progress at row 399\n",
      "Processed 400/3000\n",
      "Processed 450/3000\n",
      "ğŸ’¾ Saved progress at row 499\n",
      "Processed 500/3000\n",
      "Processed 550/3000\n",
      "ğŸ’¾ Saved progress at row 599\n",
      "Processed 600/3000\n",
      "Processed 650/3000\n",
      "ğŸ’¾ Saved progress at row 699\n",
      "Processed 700/3000\n",
      "âš ï¸ Error at row 702: No valid JSON found in model output: ...\n",
      "Processed 750/3000\n",
      "ğŸ’¾ Saved progress at row 800\n",
      "Processed 800/3000\n",
      "Processed 850/3000\n",
      "ğŸ’¾ Saved progress at row 900\n",
      "Processed 900/3000\n",
      "Processed 950/3000\n",
      "ğŸ’¾ Saved progress at row 1000\n",
      "Processed 1000/3000\n",
      "Processed 1050/3000\n",
      "ğŸ’¾ Saved progress at row 1100\n",
      "Processed 1100/3000\n",
      "Processed 1150/3000\n",
      "ğŸ’¾ Saved progress at row 1200\n",
      "Processed 1200/3000\n",
      "Processed 1250/3000\n",
      "âš ï¸ Error at row 1272: No valid JSON found in model output: ...\n",
      "Processed 1300/3000\n",
      "ğŸ’¾ Saved progress at row 1301\n",
      "Processed 1350/3000\n",
      "Processed 1400/3000\n",
      "ğŸ’¾ Saved progress at row 1401\n",
      "Processed 1450/3000\n",
      "Processed 1500/3000\n",
      "ğŸ’¾ Saved progress at row 1501\n",
      "Processed 1550/3000\n",
      "Processed 1600/3000\n",
      "ğŸ’¾ Saved progress at row 1601\n",
      "Processed 1650/3000\n",
      "Processed 1700/3000\n",
      "ğŸ’¾ Saved progress at row 1701\n",
      "Processed 1750/3000\n",
      "Processed 1800/3000\n",
      "ğŸ’¾ Saved progress at row 1801\n",
      "Processed 1850/3000\n",
      "Processed 1900/3000\n",
      "ğŸ’¾ Saved progress at row 1901\n",
      "Processed 1950/3000\n",
      "Processed 2000/3000\n",
      "ğŸ’¾ Saved progress at row 2001\n",
      "Processed 2050/3000\n",
      "Processed 2100/3000\n",
      "ğŸ’¾ Saved progress at row 2101\n",
      "Processed 2150/3000\n",
      "Processed 2200/3000\n",
      "ğŸ’¾ Saved progress at row 2201\n",
      "Processed 2250/3000\n",
      "Processed 2300/3000\n",
      "ğŸ’¾ Saved progress at row 2301\n",
      "Processed 2350/3000\n",
      "âš ï¸ Error at row 2358: No valid JSON found in model output: ...\n",
      "Processed 2400/3000\n",
      "ğŸ’¾ Saved progress at row 2402\n",
      "Processed 2450/3000\n",
      "Processed 2500/3000\n",
      "ğŸ’¾ Saved progress at row 2502\n",
      "Processed 2550/3000\n",
      "Processed 2600/3000\n",
      "ğŸ’¾ Saved progress at row 2602\n",
      "Processed 2650/3000\n",
      "Processed 2700/3000\n",
      "ğŸ’¾ Saved progress at row 2702\n",
      "Processed 2750/3000\n",
      "Processed 2800/3000\n",
      "ğŸ’¾ Saved progress at row 2802\n",
      "Processed 2850/3000\n",
      "Processed 2900/3000\n",
      "ğŸ’¾ Saved progress at row 2902\n",
      "Processed 2950/3000\n",
      "âœ… Done. All rows processed.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T17:04:21.610526Z",
     "start_time": "2026-02-19T17:04:21.159833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "for m in client.models.list():\n",
    "    print(m.id)"
   ],
   "id": "f86c36fec80800b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-sonnet-4-6\n",
      "claude-opus-4-6\n",
      "claude-opus-4-5-20251101\n",
      "claude-haiku-4-5-20251001\n",
      "claude-sonnet-4-5-20250929\n",
      "claude-opus-4-1-20250805\n",
      "claude-opus-4-20250514\n",
      "claude-sonnet-4-20250514\n",
      "claude-3-haiku-20240307\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
